---
title: "13-ML_DeepLearningPython4"
author: "PoMingChen"
date: "4/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 4.1 Four branches of machine learning

### Supervised Learning 

binary or mulitclass classification

### UNsupervised Learning 

> Unsupervised learning is the bread and butter of data analytics, and it’s often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. __Dimensionality reduction__ and __clustering__ are well-knowncategories of unsupervised learning.

### Self-supervised Learning 

Self-supervised learning is supervised learning without human-annotated labels

### Reinforcement Learning

> Currently, reinforcement learning is mostly a research area and hasn’t yet had sig-nificant practical successes beyond games. In time, however, we expect to see rein-forcement learning take over an increasingly large range of real-world applications:self-driving cars, robotics, resource management, education, and so on. It’s an ideawhose time has come, or will come soon.


### Some key point

- sample(input)

- prediction(output)

- target(the truth you want to realize)

- label

> A specific instance of a class annotation in a classification problem.For instance, if picture #1234 is annotated as containing the class “dog,”then “dog” is a label of picture #1234.

- Multilabel classification

> A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the“dog” label. The number of labels per image is usually variable

- Vector regression

> (Opposite from the scalar regression) A task where the target is a set of continuous values: forexample, a continuous vector. If you’re doing regression against multiple val-ues (such as the coordinates of a bounding box in an image), then you’redoing vector regression

- Mini-batch or batch

> A small set of samples (typically between 8 and 128)that are processed simultaneously by the model. The number of samples isoften a power of 2, to facilitate memory allocation on GPU. When training, amini-batch is used to compute a single gradient-descent update applied tothe weights of the model.


## 4.2  Evaluating machine-learning models

does the model perfom well? Is it able to generalized? 

### 4.2.1    Training, validation, and test sets

there are three classic evaluation recipe 

#### simple outvalidation

potential flaw is that the validation set and test set may not statistically representative of the data.

__how to justify the problem?__ if different randomshuffling rounds of the data before splitting end up yielding very different measuresof model performance, then you’re having this issue

if different randomshuffling rounds of the data before splitting end up yielding very different measuresof model performance, then you’re having this issue. 

#### K- fold validation
