---
title: "13-ML_DeepLearningPython4"
author: "PoMingChen"
date: "4/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 4.1 Four branches of machine learning

### Supervised Learning 

binary or mulitclass classification

### UNsupervised Learning 

> Unsupervised learning is the bread and butter of data analytics, and it’s often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. __Dimensionality reduction__ and __clustering__ are well-knowncategories of unsupervised learning.

### Self-supervised Learning 

Self-supervised learning is supervised learning without human-annotated labels

### Reinforcement Learning

> Currently, reinforcement learning is mostly a research area and hasn’t yet had sig-nificant practical successes beyond games. In time, however, we expect to see rein-forcement learning take over an increasingly large range of real-world applications:self-driving cars, robotics, resource management, education, and so on. It’s an ideawhose time has come, or will come soon.


### Some key point

- sample(input)

- prediction(output)

- target(the truth you want to realize)

- label

> A specific instance of a class annotation in a classification problem.For instance, if picture #1234 is annotated as containing the class “dog,”then “dog” is a label of picture #1234.

- Multilabel classification

> A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the“dog” label. The number of labels per image is usually variable

- Vector regression

> (Opposite from the scalar regression) A task where the target is a set of continuous values: forexample, a continuous vector. If you’re doing regression against multiple val-ues (such as the coordinates of a bounding box in an image), then you’redoing vector regression

- Mini-batch or batch

> A small set of samples (typically between 8 and 128)that are processed simultaneously by the model. The number of samples isoften a power of 2, to facilitate memory allocation on GPU. When training, amini-batch is used to compute a single gradient-descent update applied tothe weights of the model.


## 4.2  Evaluating machine-learning models

does the model perfom well? Is it able to generalized? 

### 4.2.1    Training, validation, and test sets

there are three classic evaluation recipe 

#### simple outvalidation

potential flaw is that the validation set and test set may not statistically representative of the data.

__how to justify the problem?__ if different randomshuffling rounds of the data before splitting end up yielding very different measuresof model performance, then you’re having this issue

if different randomshuffling rounds of the data before splitting end up yielding very different measuresof model performance, then you’re having this issue. 

#### K- fold validation

#### iterated K-Fold validation with shuffling

> applying K-fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training andevaluating P × K models (where P is the number of iterations you use)

### 4.2.2 Things to keep in mind

- Data representativeness 

the training set and test set need to be representative of the dataset.

> you usually should __randomly shuffle__ your data before splitting it into training and test sets.

For example, you have the sample array for class(0~10), you should never put 80% above as training, or your test set will be all data within 9~10 class. (horrible)

- the arrow of time

for the weather forecast or stock market, or the time series data. You should not random the dataset for training and test.

You should make sure all data in your test set is __posterior__ to the data in the training set.

- redundancy for your data

If some data points in your data appear twice (fairly common with real-world data), then shuffling the data and splitting it into a training set and a validation set will result in redundancy(多餘的) between the training and validation set.

make sure the training set and validation set is disjoint.


## 4.3 Data preprocessing, feature engineering, and feature learning

For now, we’ll review the basics that are common to all datadomains.

### 4.3.1 Data preprocessing for neural networks

This includes `vectorization`, `normalization`, `handling missing values`, and `feature extraction`

- vectorization

Whatever data you need to process—sound,images, text—you must first __turn into tensors(in floating point data or integer)__, a step called data vectorization.

- normalization

> take small values and be homogenoeus(all features should take values in roughly the same range)

> sometimes it's also helpful to normalize to mean-0 sd-1

some features had small floating-point values, others had fairly large integer values. Before you fed this data into your network,you had to normalize each feature __independently__ so that it had a standard deviation of 1 and a mean of 0.

- handling the missing value

### 4.3.2 feature engineering

Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before it goes into the model. 

舉例：想要判斷現在幾點？

給照片像素深淺，未若於給指針相對位置，未若於給指針之於圓中央的角度。(page 125 figure 4.3)

That’s the essence of feature engineering: making a problem easier by expressingit in a simpler way. It usually requires understanding the problem in depth.

> validation set是用來決定模型結構的。那training set，是用來決定模型 $\beta$ 的參數值。但是validation則是用來決定整個最終的模型結構要有幾個feature（X變數）放到迴歸式中。

> 關於validation set，確實只有如此嗎？有沒有一個實際的例子。


## 4.4 Overfitting and underfitting

1. Overfitting happens in everymachine-learning problem. Learning how to deal with overfitting is essential to mas-tering machine learning

2.  the lower the loss on training data, the lower the loss on test data. While this is happening, your model is said to be __underfit__: there is still progress to be made; the network hasn’t yet modeled all relevant patterns in the training data.

3. To prevent a model from learning misleading or irrelevant patterns found in thetraining data, the best solution is to get more training data. 

4. The processing of fighting overfitting this way is called __regularization__. 

Let's show some useful technique in regularization.

- Get more training data(the best solution, if not, see the three techniques below)
- Reduce the capacity of the network
- Add weight regularization
- Add dropout

### 4.4.1 Reducing the network's size

1. too much capacity and not enough capacity is an art

意思是指不要讓你的深度學習模型的layer and unit of layer過高，因為你讓模型過太爽，他可以輕鬆地將資料大量的保留。但是卻不易魚一班話，這樣面對新的資料及時候，很難表現得好。

同時不要把你的模型容納空間資訊壓得太小，這樣會有uderfitting的問題，你的模型還沒有捕捉玩所有的資料變異，就要進到下一層的layer了，已經被篩掉的資料特性是不會被保留的 。

2. The general workflow to find an appropriate modelsize is to start with relatively few layers and parameters, and increase the size of the layers or add new layers until you see diminishing returns with regard to validation loss.

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

用比較少的network來做做看，並且把他當成benchmark model，進行近一步的優化.
```{python}
model = models.Sequential()
model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```


### 4.4.2 Adding weight regularization

It’s done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:

1. __L1 regularization__: The cost added is proportional to the absolute value of the weight coefficients
2. __L2 regularization__: The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights). L2 regularization is also called __weight decay__ in the context of neural networks. Don’t let the different name confuse you: weight decay is mathematically the same as L2 regularization

```{python}
from keras import regularizers
model = models.Sequential()
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

### 4.4.3 Adding Dropout

1. The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5

(dropout就是將每一層輸出的結果，以一定比例將其歸零。用意是降低到此為止的資訊濃縮度，藉此降低過度配適問題。)

2.  At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time

在訓練資料集裡面，我們會採取dropout，但是在test data，我們不會以dropout這個動作。但是我們會將這個layer所輸出的資訊量，以跟dropout rate相同比率的前提下，將其規模縮小。（好比說輸出資訊量有10個浮點數數字，我會依照比例0.2去掉兩個，剩下八個，而不是將其歸零。

```{python}
layer_output *= np.random.randint(0, high=2, size=layer_output.shape)
# At training time, drops out 50% of the units in the output
```

```{python}
layer_output *= 0.5
#At test time
```

3. The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren’t significant

透過隨機將訓練資料集的訊息歸零，用意是藉此破壞資料與資料之間不顯著的關聯性（Hinton形容其為conspiracies，共謀關係），來避免過度配適。

```{python}
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dropout(0.5)) #layer.Dropout
model.add(layers.Dense(1, activation='sigmoid'))
```

