---
title: "13-ML_DeepLearningPython4"
author: "PoMingChen"
date: "4/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 4.1 Four branches of machine learning

### Supervised Learning 

binary or mulitclass classification

### UNsupervised Learning 

> Unsupervised learning is the bread and butter of data analytics, and it’s often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. __Dimensionality reduction__ and __clustering__ are well-knowncategories of unsupervised learning.

### Self-supervised Learning 

Self-supervised learning is supervised learning without human-annotated labels

### Reinforcement Learning

> Currently, reinforcement learning is mostly a research area and hasn’t yet had sig-nificant practical successes beyond games. In time, however, we expect to see rein-forcement learning take over an increasingly large range of real-world applications:self-driving cars, robotics, resource management, education, and so on. It’s an ideawhose time has come, or will come soon.


### Some key point

- sample(input)

- prediction(output)

- target(the truth you want to realize)

- label

> A specific instance of a class annotation in a classification problem.For instance, if picture #1234 is annotated as containing the class “dog,”then “dog” is a label of picture #1234.

- Multilabel classification

> A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the“dog” label. The number of labels per image is usually variable

- Vector regression

> (Opposite from the scalar regression) A task where the target is a set of continuous values: forexample, a continuous vector. If you’re doing regression against multiple val-ues (such as the coordinates of a bounding box in an image), then you’redoing vector regression

- Mini-batch or batch

> A small set of samples (typically between 8 and 128)that are processed simultaneously by the model. The number of samples isoften a power of 2, to facilitate memory allocation on GPU. When training, amini-batch is used to compute a single gradient-descent update applied tothe weights of the model.


## 4.2  Evaluating machine-learning models

does the model perfom well? Is it able to generalized? 

### 4.2.1    Training, validation, and test sets

there are three classic evaluation recipe 

#### simple outvalidation

potential flaw is that the validation set and test set may not statistically representative of the data.

__how to justify the problem?__ if different randomshuffling rounds of the data before splitting end up yielding very different measuresof model performance, then you’re having this issue

if different randomshuffling rounds of the data before splitting end up yielding very different measuresof model performance, then you’re having this issue. 

#### K- fold validation

#### iterated K-Fold validation with shuffling

> applying K-fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training andevaluating P × K models (where P is the number of iterations you use)

### 4.2.2 Things to keep in mind

- Data representativeness 

the training set and test set need to be representative of the dataset.

> you usually should __randomly shuffle__ your data before splitting it into training and test sets.

For example, you have the sample array for class(0~10), you should never put 80% above as training, or your test set will be all data within 9~10 class. (horrible)

- the arrow of time

for the weather forecast or stock market, or the time series data. You should not random the dataset for training and test.

You should make sure all data in your test set is __posterior__ to the data in the training set.

- redundancy for your data

If some data points in your data appear twice (fairly common with real-world data), then shuffling the data and splitting it into a training set and a validation set will result in redundancy(多餘的) between the training and validation set.

make sure the training set and validation set is disjoint.


## 4.3 Data preprocessing, feature engineering, and feature learning

For now, we’ll review the basics that are common to all datadomains.

### 4.3.1 Data preprocessing for neural networks

This includes `vectorization`, `normalization`, `handling missing values`, and `feature extraction`

- vectorization

Whatever data you need to process—sound,images, text—you must first __turn into tensors(in floating point data or integer)__, a step called data vectorization.

- normalization

> take small values and be homogenoeus(all features should take values in roughly the same range)

> sometimes it's also helpful to normalize to mean-0 sd-1

some features had small floating-point values, others had fairly large integer values. Before you fed this data into your network,you had to normalize each feature __independently__ so that it had a standard deviation of 1 and a mean of 0.

- handling the missing value

### 4.3.2 feature engineering

Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before it goes into the model. 

舉例：想要判斷現在幾點？

給照片像素深淺，未若於給指針相對位置，未若於給指針之於圓中央的角度。(page 125 figure 4.3)

That’s the essence of feature engineering: making a problem easier by expressingit in a simpler way. It usually requires understanding the problem in depth.



