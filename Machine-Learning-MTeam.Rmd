--- 
title: "A Minimal ML Example"
author: "M-Team"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
description: "This is a minimal example of using the bookdown package to build-up a ML reference book. The output format for this example is bookdown::gitbook."
---

# Basic tutorial

Placeholder


## 新增章節方式
## 環境設定
## 資料集引入

<!--chapter:end:index.Rmd-->


# (PART) Part I: Supervised-Learning{-}

Placeholder


## check distribution after normalization{-}

<!--chapter:end:01-ML_ReRegression.Rmd-->

# k-nearest Neighbors

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
knn_opts = data.frame(k=c(seq(3, 11, 2), 25, 51, 101))
knn_opts

results_knn = train(quality~., 
                    data=wine_train, 
                    method='knn',
                    preProcess=c('center', 'scale'), 
                    trControl=cv_opts,
                    tuneGrid = knn_opts)

results_knn
```

```{r}
preds_knn = predict(results_knn, wine_test)
confusionMatrix(preds_knn, good_observed, positive='good')
```

<!--chapter:end:02-ML_kNN.Rmd-->


# Neutral Network

Placeholder


## 機器學習步驟{-}
### 流程圖{-}
#### CPU平行運算{-}
### 切割資料{-}
### How the model is evaluated{-}
### Pre-processing setup{-}
#### What kind of data transformation is needed for the algorithm?{-}

<!--chapter:end:03-ML_NNetwork.Rmd-->


# Neutral Network2

Placeholder


## 機器學習步驟{-}
### 流程圖{-}
#### 啟用CPU平行運算{-}
#### 資料處理{-}
## 類神經網絡(NNet){-}
### How the model is evaluated{-}
### Pre-processing setup{-}  
#### What kind of data transformation is needed for the algorithm?{-}
### (Tuning) parameter set setup{-}
#### What are the tuning parameters{-}

<!--chapter:end:04-ML_NNetwork2.Rmd-->


# Trees and Forests

Placeholder


### 概念{-}
#### Decision tree classifier{-}
#### Random forest classifier{-}
### Cross-Validation & Pre processing{-}
### Tuning parameters{-}
### 各別變數的重要性{-}
#### 基本概念{-}
#### 重要性與樹的結構關連{-}
#### 4.3 交叉項的重要性{-}
### LIME{-}
#### 圖形解釋{-}
#### 示範程式{-}

<!--chapter:end:05-ML_TreeForest1.Rmd-->


# Trees and Forests2

Placeholder


## 隨機森林{-}
### 概念{-}
#### Decision tree classifier{-}
#### Random forest classifier{-}
### Cross-Validation & Pre processing{-}
### Tuning parameters{-}
### make confusionm matrix{-}
### 變數重要性衡量{-}
#### VIMP{-}
#### Minimal depth{-}
#### Other Measures{-}
### 觀察兩兩變數之關係{-}
### LIME{-}

<!--chapter:end:06-ML_TreeForest2.Rmd-->


# SVM 

Placeholder


## 概念{-}
#### Algorithm Summary{-}
#### Kernel Function{-}
### The Non-Separable case{-}
## R-code example{-}
### Cross-Validation & Pre processing{-}
### Tuning parameters{-}

<!--chapter:end:07-ML_SVM.Rmd-->


# (PART) Part II: Unsupervised-Learning{-}

Placeholder


## Cluster{-}
### K-平均演算法{-}
#### 概念{-}
#### 演算法描述{-}
#### 優缺點{-}
## Latent Variable Models{-}
### 常見的分析手法{-}
#### 背後關鍵的處理原則{-}
#### PCA示範{-}
## Graphical Structure{-}
## Imputation{-}

<!--chapter:end:08-ML_DeepLearning.Rmd-->


# Ensembles

Placeholder


### Bagging{-}
### Boosting{-}
#### R Code Example{-}
### Stacking{-}

<!--chapter:end:09-ML_Ensembles.Rmd-->

# (PART) Part III: Deep-Learning{-}

# Deep Learning

>因為較難訓練這類的模型，所以一個模型就會被用到許多種情況上，這時候數據具有「comparable」的特性很重要。

深度學習廣泛地運用在AI技術、臉部辨識、電腦視覺、語音辨識、自然語言處理（NLP）...。常用的技術包含前饋神經網路（Feed Forward Network）、卷積神經網路（Convolutional Neural Network）、循環神經網絡（Recurrent Neural Networks）、結構遞歸神經網絡（Recursive Neural Networks），經過前幾章已經了解了基本的神經網路外，下一步便是可以去學習以上這些神經網路。

- Python套件：tensorflow、pytorch、keras。

- R套件：sparklyr、keras。

此書並未深入介紹深度學習，而是另外推薦 [深度學習的網站](http://deeplearning.net/)，作者只有將R及Python的小範例放在附件中。

<!--chapter:end:10-ML_DeepLearning.Rmd-->


# Summary 

Placeholder


## Feature Selection & Importance
## Natural Language Processing/Text Analysis
## Bayesian Approaches
## More Stuff
## Cautionary Notes
## Some Guidelines

<!--chapter:end:11-ML_Summary.Rmd-->

# (APPENDIX) Appendix {-}

# Offical tutorial

1. [bookdown repo](https://github.com/rstudio/bookdown)

2. [bookdown webpage](https://bookdown.org/yihui/bookdown/)

3. For your convenience, you may open the minimal bookdown template in new project in RStudio whenever you want to write down a book. 

<!--chapter:end:Appendix_01_bookdown.Rmd-->

# The Dataset

```{r}
wine <- read.csv("~/Dropbox/M-Team/ML/wine.csv")
str(wine)
```

```{r}
set.seed(1234) # so that the indices will be the same when re-run
# 抽出80%樣本來train, output format is matrix
trainIndices = createDataPartition(wine$quality, p=.8, list=F) 

# delete highly correlated free.sulfur and density
wine_train = wine %>% 
  select(-free.sulfur.dioxide, -density, -color, -white) %>% 
  dplyr::slice(trainIndices)

wine_test = wine %>% 
  select(-free.sulfur.dioxide, -density, -color, -white) %>% 
  dplyr::slice(-trainIndices)
```

<!--chapter:end:Appendix_02_dataset.Rmd-->


# Python on RStudio

Placeholder


##### 找到Rprofile.site的R根目錄{-}
#### 未來開python環境用法{-}
#### Google API Installation{-}

<!--chapter:end:Appendix_03_reticulate.Rmd-->


# ML_VV

Placeholder


##### Regularized Regression{-}
##### k-nearest Neighbors{-}
##### Neural networks{-}
##### Support Vector Machines{-}
##### kernel function{-}
##### example{-}  

<!--chapter:end:Appendix_04_ML_VV.Rmd-->


# Environment setup{-}

Placeholder


## 引入資料（The IMDB dataset）
### Loading the IMDB dataset
## 處理資料（Preparing the data）
### Encoding the integer sequences into a binary matrix
## 建神經網絡（Building your network）
### The model definition
### Compiling the model
### Configuring the optimizer
### Using custom losses and metrics
## 驗證（Validating your approach）
### Setting aside a validation set
### Training your model
### Plotting the training and validation loss
### Plotting the training and validation accuracy
### Retraining a model from scratch
## 預測（Using a trained network to generate predictions on new data）
## 可調參數 （Further experiments）
## 引入資料（The Reuters dataset）
###  Loading the Reuters dataset
## 處理資料（Preparing the data）
### Encoding the data
### One-hot encoding
## 建神經網絡（Building your network）
### Model definition
### Compiling the model
## 驗證
### Setting aside a validation set
### Training the model
### Plotting the training and validation loss
### Plotting the training and validation accuracy
### Retraining a model from scratch
## 預測（Using a trained network to generate predictions on new data）
## 可調參數 （Further experiments）
## 其他課題
### A different way to handle the labels and the loss
### The importance of having sufficiently large intermediate layers
## 引入資料（The Boston Housing Price dataset）
###  Loading the Boston housing dataset

<!--chapter:end:DL_Jane-martin.Rmd-->

---
title: "Deep Learning"
author: "陳宜榛"
date: "3/19/2019"
output: html_document
---
# Environment setup{-}

```{r}
library(reticulate)
use_python("~/anaconda3/envs/m-team-machine-learning/bin/python",
           required = T)
use_condaenv("m-team-machine-learning")
```


```{r, eval=FALSE}
conda_install(envname="m-team-machine-learning",
              c("keras","tensorflow","pip"),
              pip=TRUE)
```

# CH3 {-}

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(784,)))
model.add(layers.Dense(10, activation='softmax'))
```

```{python}
input_tensor = layers.Input(shape=(784,))
x = layers.Dense(32, activation='relu')(input_tensor)
output_tensor = layers.Dense(10, activation='softmax')(x)
model = models.Model(inputs=input_tensor, outputs=output_tensor)
```

```{python}
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss='mse',
metrics=['accuracy'])
```

# 3.4 二元分類範例（Classifying movie reviews:a binary classification example）

根據電影評論的文字內容，分類正面或負面的評論。

## 引入資料（The IMDB dataset）

來自電影評論網站的50,000個評論。

### Loading the IMDB dataset

```{python}
from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
num_words=10000) # 在資料中留下前10,000個最常出現的詞
```

```{python}
# train_data and test_data are lists of reviews： each review is a list of word indices (encoding a sequence of words)
print(train_data[0])
```

```{r}
ps$train_data -> train_data
py$train_labels->train_labels # 1 正評； 0 負評
```

```{python}
# train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive
print(train_labels[0])
```

```{python}
# no word index will exceed 10,000
print(max([max(sequence) for sequence in train_data]))
```

```{python}
# how you can quickly decode one of these reviews back to English words?
word_index = imdb.get_word_index()
reverse_word_index = dict(
[(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join(
[reverse_word_index.get(i - 3, '?') for i in train_data[0]]) 
# the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
```

<https://www.w3schools.com/python/ref_dictionary_get.asp>

```{python}
help("reverse_word_index.get()")
```

```{r}
library(dplyr)
py$word_index %>% View
py$reverse_word_index
```

## 處理資料（Preparing the data）

### Encoding the integer sequences into a binary matrix

Turn the lists into tensors:

- Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the Embedding layer, which we’ll cover in detail later in the book).

```{r}
py$train_data # this is a tensor example.
```

- **One-hot encode** your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a Dense layer, capable of handling floating-point vector data.

```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension)) # Creates an all-zero matrix of shape (len(sequences), dimension)
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1. # Sets specific indices of results[i] to 1s
  return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
```

```{python}
print(x_train[0])
```

## 建神經網絡（Building your network）

### The model definition
 
- How many layers to use?

Two intermediate layers with 16 hidden units each
 
- How many hidden units to choose for each layer?

A third layer that will output the scalar prediction regarding the sentiment of the current review

在第四章才會解釋如何決定

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu')) # Two intermediate layers with 16 hidden units each
model.add(layers.Dense(1, activation='sigmoid')) # A third layer that will output the scalar prediction regarding the sentiment of the current review
```

NOTE:激活函數（activation functions）負責爲神經網絡引入非線性特徵。

[relu activation function](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)

### Compiling the model

- loss funtion:也可選擇mean_squared_error，但在二元分類中，binary_crossentropy是較好的選擇。

Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.

```{python}
model.compile(optimizer='rmsprop',
loss='binary_crossentropy',
metrics=['accuracy'])
```

### Configuring the optimizer

自行選擇optimizer的方式

```{python}
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss='binary_crossentropy',
metrics=['accuracy'])
```

### Using custom losses and metrics

自行選擇metrics的方式

```{python}
from keras import optimizers
from keras import losses
from keras import metrics
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss=losses.binary_crossentropy,
metrics=[metrics.binary_accuracy])
```

```{r}
py$metrics %>% {.$binary_accuracy}
```

## 驗證（Validating your approach）

### Setting aside a validation set

```{python}
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```

```{r}
py$x_val %>% class # it's a matrix
py$x_val %>% dim
py$y_val %>% dim
py$x_val -> x_val
x_val[1,]
```


### Training your model

```{python}
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

```{python}
help("model.fit()")
```

```{python}
history_dict = history.history
print(history_dict.keys())
```

### Plotting the training and validation loss

```{r, eval=FALSE}
conda_install("m-team-machine-learning",
  packages = "matplotlib")
```

```{r}
py$history_dict %>% names
```

```{python}
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(1, len(history_dict['acc']) + 1)

plt.plot(epochs, loss_values, 'bo', label='Training loss') # bo:blue dot
plt.plot(epochs, val_loss_values, 'b', label='Validation loss') # b:solid blue line
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```

### Plotting the training and validation accuracy

```{python}
plt.clf() # Clears the figure

acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']

plt.plot(epochs, acc_values, 'bo', label='Training acc')
plt.plot(epochs, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```

### Retraining a model from scratch

```{python}
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
loss='binary_crossentropy',
metrics=['accuracy'])
model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)
```

```{r, eval=FALSE}
py$x_train %>% class
py$x_train -> x_train
x_train[1,1:50]
```

```{python}
print(results)
```


## 預測（Using a trained network to generate predictions on new data）

```{python}
print(model.predict(x_test))
```

## 可調參數 （Further experiments）

- Try using one or three hidden layers, and see how doing so affects validation and test accuracy.

- Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.

- Try using the `mse` loss function instead of `binary_crossentropy`.

- Try using the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.

# 3.5 多元分類範例（Classifying newswires:a multiclass classification example）

根據路透社在1986年出版的一些新聞，分類為46個主題。

## 引入資料（The Reuters dataset）

###  Loading the Reuters dataset

```{python}
from keras.datasets import reuters
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(
num_words=10000) #在資料中留下前10,000個最常出現的詞
```

```{python}
# 8,982 training examples
print(len(train_data))
```

```{python}
# 2,246 test examples
print(len(test_data))
```

```{python}
# each example is a list of integers (word indices)
print(train_data[10])
```

```{python}
# how you can decode it back to words?
word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in
  train_data[0]]) # the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
```

```{python}
print(train_labels[10])
```

## 處理資料（Preparing the data）

### Encoding the data

```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

### One-hot encoding

獨熱編碼，又稱一位有效編碼，其方法是使用N位狀態暫存器來對N個狀態進行編碼，每個狀態都有它獨立的暫存器位，並且在任意時候，其中只有一位有效。

```{python}
# 自己寫的方法
def to_one_hot(labels, dimension=46):
  results = np.zeros((len(labels), dimension))
  for i, label in enumerate(labels):
    results[i, label] = 1.
  return results
one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)
```

```{python}
# keras內建的方法
from keras.utils.np_utils import to_categorical
one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
```

```{r}
py$one_hot_test_labels %>% class
py$one_hot_test_labels -> one_hot_test_labels
one_hot_test_labels %>% dim
one_hot_test_labels %>% head
```

## 建神經網絡（Building your network）

### Model definition

- You end the network with a `Dense` layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.

- The last layer uses a `softmax` activation. It means the network will output a probability distribution over the 46 different output classes—for every input sample, the network will produce a 46-dimensional output vector, where output[i] is the probability that the sample belongs to class i. The 46 scores will sum to 1.

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax')) # softmax:the network will output a probability distribution over the 46 different output classes
```

### Compiling the model

```{python}
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
```

## 驗證

### Setting aside a validation set

```{python}
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
```

### Training the model

```{python}
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

### Plotting the training and validation loss

```{python}
import matplotlib.pyplot as plt
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### Plotting the training and validation accuracy

```{python}
plt.clf()
acc = history.history['acc']
val_acc = history.history['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### Retraining a model from scratch

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(partial_x_train,
partial_y_train,
epochs=9,
batch_size=512,
validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)
```

```{python}
print(results)
```

```{python}
import copy
test_labels_copy = copy.copy(test_labels)
np.random.shuffle(test_labels_copy)
hits_array = np.array(test_labels) == np.array(test_labels_copy)
print(float(np.sum(hits_array)) / len(test_labels))
```

## 預測（Using a trained network to generate predictions on new data）

```{python}
predictions = model.predict(x_test)
```

```{python}
# Each entry in predictions is a vector of length 46
print(predictions[0].shape)
```

```{python}
# The coefficients in this vector sum to 1
print(np.sum(predictions[0]))
```

```{python}
# The largest entry is the predicted class—the class with the highest probability
print(np.argmax(predictions[0]))
```
從0開始數，所以3代表第四類

## 可調參數 （Further experiments）

- Try using larger or smaller layers: 32 units, 128 units, and so on.

- Try using a single hidden layer, or three hidden layers.

## 其他課題

### A different way to handle the labels and the loss

```{python}
y_train = np.array(train_labels)
y_test = np.array(test_labels)
```

The only thing this approach would change is the choice of the loss function.

```{python}
model.compile(optimizer='rmsprop',
loss='sparse_categorical_crossentropy',
metrics=['acc'])
```

### The importance of having sufficiently large intermediate layers

drop掉太多內容，會導致準確率下降

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(partial_x_train,
partial_y_train,
epochs=20,
batch_size=128,
validation_data=(x_val, y_val))
```

# 3.6 迴歸範例（Predicting house prices: a regression example）

## 引入資料（The Boston Housing Price dataset）

###  Loading the Boston housing dataset

<!--chapter:end:DL_Jane.Rmd-->



## 非監督式學習（Unsupervised Learning）
### 1. 概念
### 2. 常使用的方法
## 分群（Clustering）
### 1. 概念
### 2. K-平均演算法
#### 2-1 概念
#### 2-2 演算法描述
#### 2-3 優缺點
### 3. 階層式分群
#### 3-1 概念
### 4. 混合模型
## 學習潛在變數模型的方法（Latent Variable Models）
## 圖形結構（Graphical Structure）
## 總結

<!--chapter:end:Unsupervised-Learning.Rmd-->

