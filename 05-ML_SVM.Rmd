# SVM 

## 概念{-}

Features: $x_i$, Outcome: $y_i\in {0,1}$, 

  * supporting hyperplane (SupH): any $w,b,\delta$ that can satisfy $w^Tx_i\geq b+\delta$ iff $y_i>0$ and $w^Tx_i\leq b-\delta$ iff $y_i<0$. The equality holds for some points in each area, i.e. at least one point **touchs** each plane. 數學上兩個supporting hyperplanes可以寫成：
  
  $$supH_1: w^Tx=b+\delta$$
  $$supH_2: w^Tx=b-\delta$$
  
  * separating hypoerplane (SepH): the hyperplane that lies in the middle of the two SupHs. 數學上Separating hyperplane可以寫成：
  
  $$w^Tx=b$$
  
  * margin: 兩個supHs的距離. 
  
目標在極大化margin，使Supporting hyperplane分割空間越清楚越好，數學上來說它等同於極小化$\|w\|$，故可以寫成：

$$min_{\{w,b,\delta\}}\ \|w\|^2$$

受限於：

$$when\ (y_i=1,x_i),\ w^Tx_i-(b+\delta)\geq 0$$
$$when\ (y_i=-1,x_i),\ w^Tx_i-(b-\delta)\leq 0$$
他們等同於：

$$\begin{eqnarray*}
when\ y_i=1,\ (w^{T}x_{i}-(b+\delta))y_{i} & \geq & 0\\
when\ y_i=-1,\ (w^{T}x_{i}-(b-\delta))y_{i} & \geq & 0
\end{eqnarray*}
$$
也就是：
$$(w^Tx_i-b)y_i-\delta \geq 0$$

#### Algorithm Summary{-}

$$min_{\{w,b,\delta\}}\ \frac{1}{2}\|w\|^2$$
$$s.t.\ (w^Tx_i-b)y_i-\delta\geq 0$$
If $(w^*,b^*,\delta^*)$ is a possible solution, $\lambda (w^*,b^*,\delta^*)$ for $\lambda>0$ will be a solution too. Therefore, we normalize $\delta=1$.

$$min_{\{w,b,\delta\}}\ \frac{1}{2}\|w\|^2$$
$$s.t.\ (w^Tx_i-b)y_i-1\geq 0$$
Then do the Lagrange Multiplier.

#### Kernel Function{-}

The whole discusstion above is about linear data, if not, we should mapping data to a higher dimension by Kernel Function.

<img src="https://m-clark.github.io/introduction-to-machine-learning/introduction-to-machine-learning_files/figure-html/svm2d-1.svg" height="500" width="500">

<img src="https://m-clark.github.io/introduction-to-machine-learning/img/svm3d.png" height="500" width="500">

$$x_i^Tx_j\rightarrow\emptyset(x_i)^T\emptyset(x_j)$$

The kernel function could be very complicated, but by inner product, it will be simple a lot.
$$K(x_i,x_j)\rightarrow\emptyset(x_i)^T\emptyset(x_j)$$
$$K(x_i,x_j)\rightarrow\exp(-\frac{\|x_i-x_j||^2}{2\sigma^2})$$

### The Non-Separable case{-}

In real world, it is hard to find a optimal separating hyperplane(OSH), we need to insert a error term to handle the case that support vectors overlap with SupHs.

$$w^Tx_i-b\leq-1+\xi_i\  \forall y_i=-1$$
$$w^Tx_i-b\geq+1-\xi_i\  \forall y_i=+1$$
$$\xi_i\geq0$$
and made a cost function("c" is the weight of cost):

$$cost = c(\sum_i\xi_i)^k$$
then

$$min\ \frac{1}{2}\|w\|^2+c(\sum_i\xi_i)^k$$
$$s.t.\ (w^Tx_i-b)y_i-1+\xi_i\geq 0$$


## R-code example{-}

### Cross-Validation & Pre processing{-}

as above

### Tuning parameters{-}

the tuneLength is the number of errors allowed

```{r}
results_svm = train(quality~., 
                    data=wine_train, 
                    method='svmLinear2',
                    preProcess=c('center', 'scale'), 
                    trControl=cv_opts, 
                    tuneLength=5,
                    probability=TRUE)  # to get probs along with classifications

results_svm
```


```{r}
preds_svm = predict(results_svm, wine_test)
confusionMatrix(preds_svm, good_observed, positive='good')
```

Reference：http://www.cmlab.csie.ntu.edu.tw/~cyy/learning/tutorials/SVM2.pdf
