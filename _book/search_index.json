[
["index.html", "A Minimal ML Example Chapter 1 Basic tutorial 1.1 新增章節方式 1.2 環境設定 1.3 資料集引入", " A Minimal ML Example M-Team 2019-03-12 Chapter 1 Basic tutorial m-clark ML 1.1 新增章節方式 bookdown是稍微改良強化的rmarkdown，可以用來生成電子書。 檔名固定為X-filename.Rmd，bookdown會自動按照阿拉伯數字(X)生成章節。 # 表示新增標題，每當需要一個新的章節，即可開一個新的Rmd，第一行都是打一個#，且無需再留yaml # 大標 ## 次標。並依此類推 {-}，在#後面加上{-}，可以不顯示標題數字。可斟酌使用。 畢竟有時候看到5.1.1.1，有點煩躁 可以直接看任意一個檔案結構作為例子即可瞭解。 之後個人開啟bookdown_username作為更改分支，並且在其中修改個人負責的檔案然後進行pull request到bookdown_only這個分支，可以減低許多版本控制衝突的問題。 生成電子書指令： bookdown::render_book(&quot;local&quot;) 可以在publish之前先本機生成，檢查一下有沒有要再改的地方。 發布電子書指令： bookdown::publish_book() 1.2 環境設定 library(caret) library(tidyverse) library(glmnet) library(class) library(randomForest) library(e1071) library(ggplot2) library(dplyr) 1.3 資料集引入 wine &lt;- read.csv(&quot;~/Dropbox/M-Team/ML/wine.csv&quot;) str(wine) ## &#39;data.frame&#39;: 6497 obs. of 15 variables: ## $ X : int 0 1 2 3 4 5 6 7 8 9 ... ## $ fixed.acidity : num 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ... ## $ volatile.acidity : num 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ... ## $ citric.acid : num 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ... ## $ residual.sugar : num 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ... ## $ chlorides : num 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ... ## $ free.sulfur.dioxide : num 11 25 15 17 11 13 15 15 9 17 ... ## $ total.sulfur.dioxide: num 34 67 54 60 34 40 59 21 18 102 ... ## $ density : num 0.998 0.997 0.997 0.998 0.998 ... ## $ pH : num 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ... ## $ sulphates : num 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ... ## $ alcohol : num 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ... ## $ quality : Factor w/ 2 levels &quot;bad&quot;,&quot;good&quot;: 1 1 1 2 1 1 1 2 2 1 ... ## $ color : Factor w/ 2 levels &quot;red&quot;,&quot;white&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ white : int 0 0 0 0 0 0 0 0 0 0 ... set.seed(1234) # so that the indices will be the same when re-run # 抽出80%樣本來train, output format is matrix trainIndices = createDataPartition(wine$quality, p=.8, list=F) # delete highly correlated free.sulfur and density wine_train = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% dplyr::slice(trainIndices) wine_test = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% dplyr::slice(-trainIndices) "],
["regularized-regression.html", "Chapter 2 Regularized Regression check distribution after normalization", " Chapter 2 Regularized Regression wine &lt;- read.csv(&quot;~/Dropbox/M-Team/ML/wine.csv&quot;) str(wine) ## &#39;data.frame&#39;: 6497 obs. of 15 variables: ## $ X : int 0 1 2 3 4 5 6 7 8 9 ... ## $ fixed.acidity : num 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ... ## $ volatile.acidity : num 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ... ## $ citric.acid : num 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ... ## $ residual.sugar : num 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ... ## $ chlorides : num 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ... ## $ free.sulfur.dioxide : num 11 25 15 17 11 13 15 15 9 17 ... ## $ total.sulfur.dioxide: num 34 67 54 60 34 40 59 21 18 102 ... ## $ density : num 0.998 0.997 0.997 0.998 0.998 ... ## $ pH : num 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ... ## $ sulphates : num 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ... ## $ alcohol : num 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ... ## $ quality : Factor w/ 2 levels &quot;bad&quot;,&quot;good&quot;: 1 1 1 2 1 1 1 2 2 1 ... ## $ color : Factor w/ 2 levels &quot;red&quot;,&quot;white&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ white : int 0 0 0 0 0 0 0 0 0 0 ... set.seed(1234) # so that the indices will be the same when re-run # 抽出80%樣本來train, output format is matrix trainIndices = createDataPartition(wine$quality, p=.8, list=F) # delete highly correlated free.sulfur and density wine_train = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% dplyr::slice(trainIndices) wine_test = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% dplyr::slice(-trainIndices) check distribution after normalization wine_trainplot = select(wine_train, -quality) %&gt;% preProcess(method=&#39;range&#39;) %&gt;% #標準化處理range =&gt; (x-min)/(max-min) predict(newdata= select(wine_train, -quality)) #利用predict函數顯示出處理好的矩陣 featurePlot(wine_trainplot, wine_train$quality, &#39;box&#39;) # cross validation 10 cv_opts = trainControl(method=&#39;cv&#39;, number=10) #定義模型訓練參數，劃分十組交叉驗證（使用repeatedcv可重複劃分） regreg_opts = expand.grid(.alpha = seq(.1, 1, length = 5), .lambda = seq(.1, .5, length = 5)) #25種組合(決定lamda重要度？) results_regreg = train(quality~., data=wine_train, method = &quot;glmnet&quot;, trControl = cv_opts, preProcess = c(&quot;center&quot;, &quot;scale&quot;), #指定數據標準化，&quot;center&quot;和&quot;scale&quot;。其中center表示預測變量減去均值 tuneGrid = regreg_opts) results_regreg #kappa是一統計量指標衡量預測值與實質的差距 ## glmnet ## ## 5199 samples ## 10 predictor ## 2 classes: &#39;bad&#39;, &#39;good&#39; ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 4679, 4680, 4679, 4679, 4679, 4679, ... ## Resampling results across tuning parameters: ## ## alpha lambda Accuracy Kappa ## 0.100 0.1 0.7255274 0.35008588 ## 0.100 0.2 0.6949431 0.24105753 ## 0.100 0.3 0.6730163 0.15106604 ## 0.100 0.4 0.6516660 0.07063484 ## 0.100 0.5 0.6364689 0.01492729 ## 0.325 0.1 0.7139856 0.31350196 ## 0.325 0.2 0.6703273 0.13726298 ## 0.325 0.3 0.6330066 0.00000000 ## 0.325 0.4 0.6330066 0.00000000 ## 0.325 0.5 0.6330066 0.00000000 ## 0.550 0.1 0.7043676 0.27703468 ## 0.550 0.2 0.6330066 0.00000000 ## 0.550 0.3 0.6330066 0.00000000 ## 0.550 0.4 0.6330066 0.00000000 ## 0.550 0.5 0.6330066 0.00000000 ## 0.775 0.1 0.6882142 0.21713151 ## 0.775 0.2 0.6330066 0.00000000 ## 0.775 0.3 0.6330066 0.00000000 ## 0.775 0.4 0.6330066 0.00000000 ## 0.775 0.5 0.6330066 0.00000000 ## 1.000 0.1 0.6630178 0.11541687 ## 1.000 0.2 0.6330066 0.00000000 ## 1.000 0.3 0.6330066 0.00000000 ## 1.000 0.4 0.6330066 0.00000000 ## 1.000 0.5 0.6330066 0.00000000 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 0.1 and lambda = 0.1. ggplot(results_regreg) alpha=mixing percentage lambda=regularization parameter preds_regreg = predict(results_regreg, wine_test) good_observed = wine_test$quality confusionMatrix(preds_regreg, good_observed, positive=&#39;good&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction bad good ## bad 197 76 ## good 279 746 ## ## Accuracy : 0.7265 ## 95% CI : (0.7014, 0.7506) ## No Information Rate : 0.6333 ## P-Value [Acc &gt; NIR] : 6.665e-13 ## ## Kappa : 0.3531 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9075 ## Specificity : 0.4139 ## Pos Pred Value : 0.7278 ## Neg Pred Value : 0.7216 ## Prevalence : 0.6333 ## Detection Rate : 0.5747 ## Detection Prevalence : 0.7897 ## Balanced Accuracy : 0.6607 ## ## &#39;Positive&#39; Class : good ## The lower bound (and p-value) suggests we are statistically predicting better than the No Information Rate (i.e., just guessing the more prevalent ‘Bad’ category) -&gt; 猜好的比猜壞的還強 confusionMatrix(preds_regreg, good_observed, positive=&#39;good&#39;, mode=&#39;prec_recall&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction bad good ## bad 197 76 ## good 279 746 ## ## Accuracy : 0.7265 ## 95% CI : (0.7014, 0.7506) ## No Information Rate : 0.6333 ## P-Value [Acc &gt; NIR] : 6.665e-13 ## ## Kappa : 0.3531 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Precision : 0.7278 ## Recall : 0.9075 ## F1 : 0.8078 ## Prevalence : 0.6333 ## Detection Rate : 0.5747 ## Detection Prevalence : 0.7897 ## Balanced Accuracy : 0.6607 ## ## &#39;Positive&#39; Class : good ## "],
["k-nearest-neighbors.html", "Chapter 3 k-nearest Neighbors", " Chapter 3 k-nearest Neighbors knn_opts = data.frame(k=c(seq(3, 11, 2), 25, 51, 101)) knn_opts ## k ## 1 3 ## 2 5 ## 3 7 ## 4 9 ## 5 11 ## 6 25 ## 7 51 ## 8 101 results_knn = train(quality~., data=wine_train, method=&#39;knn&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneGrid = knn_opts) results_knn ## k-Nearest Neighbors ## ## 5199 samples ## 10 predictor ## 2 classes: &#39;bad&#39;, &#39;good&#39; ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 4679, 4679, 4679, 4680, 4679, 4679, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 3 0.7670720 0.4901107 ## 5 0.7661112 0.4848680 ## 7 0.7620724 0.4740174 ## 9 0.7601512 0.4700814 ## 11 0.7609215 0.4691504 ## 25 0.7493794 0.4373009 ## 51 0.7470705 0.4273110 ## 101 0.7401438 0.4042036 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 3. preds_knn = predict(results_knn, wine_test) confusionMatrix(preds_knn, good_observed, positive=&#39;good&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction bad good ## bad 296 147 ## good 180 675 ## ## Accuracy : 0.7481 ## 95% CI : (0.7235, 0.7715) ## No Information Rate : 0.6333 ## P-Value [Acc &gt; NIR] : &lt; 2e-16 ## ## Kappa : 0.4496 ## Mcnemar&#39;s Test P-Value : 0.07679 ## ## Sensitivity : 0.8212 ## Specificity : 0.6218 ## Pos Pred Value : 0.7895 ## Neg Pred Value : 0.6682 ## Prevalence : 0.6333 ## Detection Rate : 0.5200 ## Detection Prevalence : 0.6587 ## Balanced Accuracy : 0.7215 ## ## &#39;Positive&#39; Class : good ## "],
["neutral-network.html", "Chapter 4 Neutral Network 機器學習步驟", " Chapter 4 Neutral Network 機器學習步驟 流程圖 CPU平行運算 開始 library(doParallel) cl = makeCluster(2) registerDoParallel(cl) 結束 stopCluster(cl) 切割資料 library(gplots) library(caret) library(tidyverse) wine &lt;- read.csv(&#39;~/Dropbox/M-team/ML/wine.csv&#39;) set.seed(1234) # so that the indices will be the same when re-run trainIndices = createDataPartition(wine$quality, p=.8, list=F) wine_train = wine %&gt;% select(-X,-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(trainIndices) wine_test = wine %&gt;% select(-X,-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(-trainIndices) wine_trainplot = select(wine_train, -quality) %&gt;% preProcess(method=&#39;range&#39;) %&gt;% predict(newdata= select(wine_train, -quality)) good_observed = wine_test$quality 參考資料： https://topepo.github.io/caret/train-models-by-tag.html 所以的模型都先看一下train-model by tag，來看一下基本設定，以及模型特有的東西。 How the model is evaluated Here we choose: k-fold Cross-validation 參考資料：https://hyp.is/lc7vUNc6EeixLm87hkeo7A/m-clark.github.io/introduction-to-machine-learning/concepts.html #10-fold CV here cv_opts = trainControl(method=&#39;cv&#39;, number=10) # cross-validation Pre-processing setup What kind of data transformation is needed for the algorithm? NNL: data requires rescaling method = “center” subtracts the mean (\\(mean(x)\\)) of the predictor’s data (again from the data in x) from the predictor values while method = “scale” divides by the standard deviation (\\(sd(x)\\)). \\[\\hat{x}_i=\\frac{x_i-mean(x)}{sd(x)}\\] tuneLength=5 表示 size,decay 是個5x5的grid空間。 results_nnet = train(quality~., data=wine_train, method=&#39;avNNet&#39;, trControl=cv_opts, tuneLength=5, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trace=F, maxit=10) results_nnet ggplot(results_nnet) ggplot(results_nnet) + labs(x=&#39;Number of Hidden Units&#39;) + scale_x_continuous(breaks = c(1,3,5,7,9)) preds_nnet = predict(results_nnet, wine_test) confusionMatrix(preds_nnet, good_observed, positive=&#39;good&#39;) #results_nnet1 = train(quality~., # data=wine_train, #method=&#39;mlpWeightDecayML&#39;, #trControl=cv_opts, #preProcess=c(&#39;center&#39;, &#39;scale&#39;), #trace=F, #maxit=10) #results_nnet1 不一定要用tuneLength由電腦選grid[值]，也可改成如下的手動設定： regreg_opts = expand.grid(size = seq(.1, 1, length = 5), decay = seq(.1, .5, length = 5)) results_regreg = train( ... tuneGrid = regreg_opts) "],
["neutral-network2.html", "Chapter 5 Neutral Network2 機器學習步驟 類神經網絡(NNet)", " Chapter 5 Neutral Network2 機器學習步驟 流程圖 啟用CPU平行運算 library(doParallel) cl = makeCluster(2) registerDoParallel(cl) #stopCluster(cl)關閉 資料處理 library(gplots) library(caret) library(tidyverse) wine &lt;- read.csv(&#39;~/Dropbox/M-team/ML/wine.csv&#39;) wine$quality=factor(wine$quality) set.seed(1234) # so that the indices will be the same when re-run trainIndices = createDataPartition(wine$quality, p=.8, list=F) wine_train = wine %&gt;% select(-X,-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(trainIndices) wine_test = wine %&gt;% select(-X,-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(-trainIndices) wine_trainplot = select(wine_train, -quality) %&gt;% preProcess(method=&#39;range&#39;) %&gt;% predict(newdata= select(wine_train, -quality)) good_observed = wine_test$quality 類神經網絡(NNet) 參考資料： https://topepo.github.io/caret/train-models-by-tag.html 所以的模型都先看一下train-model by tag，來看一下基本設定，以及模型特有的東西。 How the model is evaluated Here we choose: k-fold Cross-validation 參考資料：https://hyp.is/lc7vUNc6EeixLm87hkeo7A/m-clark.github.io/introduction-to-machine-learning/concepts.html 10-fold CV here cv_opts = trainControl(method=&#39;cv&#39;, number=10) # cross-validation Pre-processing setup What kind of data transformation is needed for the algorithm? NNL: data requires rescaling method = “center” subtracts the mean (\\(mean(x)\\)) of the predictor’s data (again from the data in x) from the predictor values while method = “scale” divides by the standard deviation (\\(sd(x)\\)). \\[\\hat{x}_i=\\frac{x_i-mean(x)}{sd(x)}\\] (Tuning) parameter set setup What are the tuning parameters size: Number of hidden units decay: 如下式數值分析的\\(\\eta\\) \\[\\theta_{i+1}=\\theta_{i}-\\eta\\frac{\\delta\\ Objectivefun(\\theta_i)}{\\delta \\theta}\\] tuneLength=5 表示 size,decay 是個5x5的grid空間。 results_nnet = train(quality~., data=wine_train, method=&#39;avNNet&#39;, trControl=cv_opts, tuneLength=5, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trace=F, maxit=10) results_nnet ggplot(results_nnet) ggplot(results_nnet) + labs(x=&#39;Number of Hidden Units&#39;) + scale_x_continuous(breaks = c(1,3,5,7,9)) preds_nnet = predict(results_nnet, wine_test) confusionMatrix(preds_nnet, good_observed, positive=&#39;good&#39;) #results_nnet1 = train(quality~., # data=wine_train, #method=&#39;mlpWeightDecayML&#39;, #trControl=cv_opts, #preProcess=c(&#39;center&#39;, &#39;scale&#39;), #trace=F, #maxit=10) #results_nnet1 不一定要用tuneLength由電腦選grid[值]，也可改成如下的手動設定： regreg_opts = expand.grid(size = seq(.1, 1, length = 5), decay = seq(.1, .5, length = 5)) results_regreg = train( ... tuneGrid = regreg_opts) "],
["trees-and-forests.html", "Chapter 6 Trees and Forests", " Chapter 6 Trees and Forests 參考資料：https://topepo.github.io/caret/train-models-by-tag.html#Random_Forest 概念 Decision tree classifier Basic concept: 非常清楚的說明 scikit code documentation Random forest classifier 給定一組training data，演算法會決定那一棵樹最適合它(?)。Random forest透過Boostrapping產生如1000個training data，每個用來找一棵最適合它的樹，最後以這1000顆樹來衡量它對真正test data的預測（採多數決）。 Cross-Validation &amp; Pre processing 同前 Tuning parameters mtry: 隨機選出來用來架構樹之節點的特徵變數個數 In addition, when splitting a node during the construction of the tree, the split that is chosen (即用什麼特徵變數來進一步分類) is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. …scikit code documentation ntree: 透過Boostrapping來產生「找樹用的」training data(一組樣本找一顆)。 rf_opts = data.frame(mtry=c(2:6)) results_rf = train(quality~., data = wine_train, method = &#39;rf&#39;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = cv_opts, tuneGrid = rf_opts, localImp = T, ntree=100) results_rf preds_rf = predict(results_rf, wine_test) preds_rf confusionMatrix(preds_rf, good_observed, positive=&#39;good&#39;) 各別變數的重要性 參考資料: https://cran.rstudio.com/web/packages/randomForestExplainer/vignettes/randomForestExplainer.html 基本概念 varImp(results_rf) 重要性與樹的結構關連 library(randomForestExplainer) plot_min_depth_distribution(results_rf$finalModel) plot_min_depth_interactions(results_rf$finalModel, k=7) 4.3 交叉項的重要性 ? 用來更加突顯某一變數帶給其他變數的重要性？ multi_imps = measure_importance(results_rf$finalModel) plot_importance_ggpairs(multi_imps) 視覺化的圖，是預測的結果，若預測結果是0.8，表示Random Forest若有1000顆樹，有800顆樹認為他是好的，可是若看到是紅色的，表示本質上他是壞的，這樣就是不成功的預測。很順利的Random Forest就會把它分個很開，很成功。 # https://arxiv.org/pdf/1501.07196 # tibble causes problem so convert wine_train to standard df. library(ggRandomForests) rf2 = rfsrc(formula = quality ~., data = data.frame(wine_train), mtry = results_rf$finalModel$mtry) gg_v = gg_variable(rf2) gg_md = gg_minimal_depth(rf2) # We want the top two ranked minimal depth variables only xvar = gg_md$topvars[1:2] plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1) LIME 每一瓶酒 Permute the data n times to create data with similar distributional properties to the original. 創造相似的酒（特徵變化要符合原始資料特徴間的統計性質，如變異及相關程度） Get similarity scores of the permuted observations to the observations you wish to explain. 依相似度要計算與原本那瓶酒的 「相似度」，之後權重用。 Make predictions with the permuted data based on the ML model. 對新樣本做ML分類。 Select m features (e.g. forward selection, lasso) best describing the complex model outcome from the permuted data. 選擇一組你想理解對ML結果影響重要的特徵。 Fit a simple model, e.g. standard regression, predicting the predictions from the ML model with the m features, where observations are weighted by similarity to the to-be-explained observations. 對摸擬樣本為如好酒的機率，對這組特徵進行加權迴歸，係數值越大的越重要。 圖形解釋 示範程式 set.seed(1234) sample_index = sample(1:nrow(wine_test), 5) sample_test = wine_test %&gt;% slice(sample_index) %&gt;% select(-good) library(lime) rf_lime = lime(wine_train, results_rf) rf_explain = explain(sample_test, rf_lime, n_features = 3, feature_select = &#39;highest_weights&#39;, labels = &#39;Good&#39;) plot_features(rf_explain) plot_explanations(rf_explain) "],
["trees-and-forests2.html", "Chapter 7 Trees and Forests2 隨機森林", " Chapter 7 Trees and Forests2 隨機森林 參考資料：https://topepo.github.io/caret/train-models-by-tag.html#Random_Forest 概念 Decision tree classifier Basic concept: 非常清楚的說明 scikit code documentation Random forest classifier 給定一組training data，演算法會決定那一棵樹最適合它(?)。Random forest透過Boostrapping產生如1000個training data，每個用來找一棵最適合它的樹，最後以這1000顆樹來衡量它對真正test data的預測（採多數決）。 Cross-Validation &amp; Pre processing 同前 Tuning parameters mtry: 隨機選出來用來架構樹之節點的特徵變數個數 In addition, when splitting a node during the construction of the tree, the split that is chosen (即用什麼特徵變數來進一步分類) is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. …scikit code documentation ntree: 透過Boostrapping來產生「找樹用的」training data(一組樣本找一顆)。 rf_opts = data.frame(mtry=c(2:6)) results_rf = train(quality~., data = wine_train, method = &#39;rf&#39;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = cv_opts, tuneGrid = rf_opts, localImp = T, ntree=10) results_rf make confusionm matrix 參考資料：https://hyp.is/f2kmRgEUEemKAFeGta_7RA/m-clark.github.io/introduction-to-machine-learning/opening-the-black-box.html preds_rf = predict(results_rf, wine_test) preds_rf confusionMatrix(preds_rf, good_observed, positive=&#39;good&#39;) 利用confusionMatrix觀察模型衡量指標（準確率、召回率．．．）並依照研究問題判斷模型適不適合。 變數重要性衡量 參考資料: https://cran.rstudio.com/web/packages/randomForestExplainer/vignettes/randomForestExplainer.html VIMP 概念：利用特徵經過置換前與置換後的誤差影響，來衡量該特徵的重要性。 步驟： 利用每棵樹的分類模型來預測自己的OOB樣本，並計算錯誤率。 OOB：在建構每棵樹的時候，我們對訓練集使用了不同的bootstrap sample。所以對於每棵樹而言，大约有1/3的資料點是沒有參與該棵樹的生成，他們就是該棵樹的OOB样本。 對想了解該特徵重要性的特徵進行隨機打亂，例如：把各資料點的「酒精濃度」進行隨機打亂。 利用原隨機森林模型進行預測得到新的outcome。 計算每棵樹新的OOB樣本錯誤率。 對於每棵樹擾亂特徵前後所得到的錯誤率相減並平均。 得出因該特徵擾亂後而導致的平均誤差上升多少，越高代表該變數越重要。 varImp(results_rf) Minimal depth 概念：每棵樹在生成每個節點時都會有一個特徵，在樹越上層（越淺）的特徵重要程度會越大，利用此特點來計算特徵的平均最小深度觀察特徵的重要性。 補充：假設森林有兩棵樹，A樹中特徵「酒精濃度」出現在第一層，B樹中「酒精濃度」出現在第二層與第四層，那麼平均最小深度為(\\(\\frac{7}{3}\\)) library(randomForestExplainer) plot_min_depth_distribution(results_rf$finalModel) plot_min_depth_interactions(results_rf$finalModel, k=7) Other Measures 參考資料：https://cran.r-project.org/web/packages/randomForestExplainer/vignettes/randomForestExplainer.html multi_imps = measure_importance(results_rf$finalModel) plot_importance_ggpairs(multi_imps) 觀察兩兩變數之關係 參考資料：ggRandomForests https://arxiv.org/pdf/1501.07196 # tibble causes problem so convert wine_train to standard df. library(ggRandomForests) rf2 = rfsrc(formula = quality ~., data = data.frame(wine_train), mtry = results_rf$finalModel$mtry) gg_v = gg_variable(rf2) gg_md = gg_minimal_depth(rf2) gg_v gg_md xvar = gg_md$topvars[1:2]#表示取出前兩個最重要的變數。 plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1) 圖中縱軸為模型判定為good的機率，每一個點代表一個資料點，顏色為該資料點的真實outcome，以點(10,0.75,紅)為例；表示有一瓶酒，其酒精濃度為10且1000棵樹裡面有750棵說他是good(0.75)，但它實際上是壞的（紅色）。 LIME LIME 想要解決的問題：找到一個容易解釋的模型 g 解釋為什麼一個個體會被分類到f預測的類別；f是依據什麼特徵來分類進一步了解各特徵的重要性。 參考資料： https://medium.com/@kstseng/lime-local-interpretable-model-agnostic-explanation-%E6%8A%80%E8%A1%93%E4%BB%8B%E7%B4%B9-a67b6c34c3f8 步驟： 每一瓶酒 Permute the data n times to create data with similar distributional properties to the original. 創造相似的酒（特徵變化要符合原始資料特徴間的統計性質，如變異及相關程度）。 擾動樣本：進行有意義的擾動（改變\\({x}_i\\)的幾個特徵值），產生新的輸入資料\\({z}_i\\)。 Get similarity scores of the permuted observations to the observations you wish to explain. 依相似度要計算與原本那瓶酒的 「相似度」；與原資料的距離越近者給予的係數 \\(\\pi_{{x}_i}\\) 越大，之後權重用。 Make predictions with the permuted data based on the ML model. 對新樣本(\\({z}_i\\))做ML分類得到新樣本得預測結果g(z)。 Min loss function : \\(\\sum\\pi_{{x}_i}(f(z)-g(z))^2+ \\Omega(g)\\) f(z)為真實outcome，\\(\\Omega(g)\\)為懲罰項目的為希望g能簡單一點，\\(\\pi_{{x}_i}\\)為「與\\({x}_i\\)相似與否」的核函數。極小化loss function 找出最適的g。 Select m features (e.g. forward selection, lasso) best describing the complex model outcome from the permuted data. 選擇一組你想理解對ML結果影響重要的特徵。 Fit a simple model, e.g. standard regression, predicting the predictions from the ML model with the m features, where observations are weighted by similarity to the to-be-explained observations. 對摸擬樣本\\({z}_i\\)與選好的幾個特徵進行加權迴歸(model g)，觀察各特徵係數值；係數值越大者越重要。 因為LIME很吃電腦資源所以下例程式碼中只隨機挑選了5個case（5個資料點）進行LIME set.seed(1234) sample_index = sample(1:nrow(wine_test), 5)#隨機選取幾個case sample_test = wine_test %&gt;% slice(sample_index) %&gt;% select(-quality) #分別拿掉5個case的outcome library(lime) rf_lime = lime(wine_train, results_rf)#lime rf_explain = explain(sample_test, rf_lime, n_features = 3,#只看三種特徵的組合 feature_select = &#39;highest_weights&#39;, labels = &#39;good&#39;) rf_explain#各case跑完lime的係數狀況 plot_features(rf_explain) plot_explanations(rf_explain) 以case 1 為例： Probablity為預測good的機率 feature_weight為-0.15表示當「0.40 &lt; volatile.acidity」時，每增加一單位酸度y便會造成減少0.16。 \\[y = \\left\\{\\begin{array}{ll} bad, &amp; \\mbox{if $y&lt;0$} \\\\ good, &amp; \\mbox{if $y&gt;0$} \\\\ \\end{array} \\right.\\] Explanation為\\(R^2\\) "],
["svm.html", "Chapter 8 SVM 概念 R-code example", " Chapter 8 SVM 概念 Features: \\(x_i\\), Outcome: \\(y_i\\in {0,1}\\), supporting hyperplane (SupH): any \\(w,b,\\delta\\) that can satisfy \\(w^Tx_i\\geq b+\\delta\\) iff \\(y_i&gt;0\\) and \\(w^Tx_i\\leq b-\\delta\\) iff \\(y_i&lt;0\\). The equality holds for some points in each area, i.e. at least one point touchs each plane. 數學上兩個supporting hyperplanes可以寫成： \\[supH_1: w^Tx=b+\\delta\\] \\[supH_2: w^Tx=b-\\delta\\] separating hypoerplane (SepH): the hyperplane that lies in the middle of the two SupHs. 數學上Separating hyperplane可以寫成： \\[w^Tx=b\\] margin: 兩個supHs的距離. 目標在極大化margin，使Supporting hyperplane分割空間越清楚越好，數學上來說它等同於極小化\\(\\|w\\|\\)，故可以寫成： \\[min_{\\{w,b,\\delta\\}}\\ \\|w\\|^2\\] 受限於： \\[when\\ (y_i=1,x_i),\\ w^Tx_i-(b+\\delta)\\geq 0\\] \\[when\\ (y_i=-1,x_i),\\ w^Tx_i-(b-\\delta)\\leq 0\\] 他們等同於： \\[\\begin{eqnarray*} when\\ y_i=1,\\ (w^{T}x_{i}-(b+\\delta))y_{i} &amp; \\geq &amp; 0\\\\ when\\ y_i=-1,\\ (w^{T}x_{i}-(b-\\delta))y_{i} &amp; \\geq &amp; 0 \\end{eqnarray*} \\] 也就是： \\[(w^Tx_i-b)y_i-\\delta \\geq 0\\] Algorithm Summary \\[min_{\\{w,b,\\delta\\}}\\ \\frac{1}{2}\\|w\\|^2\\] \\[s.t.\\ (w^Tx_i-b)y_i-\\delta\\geq 0\\] If \\((w^*,b^*,\\delta^*)\\) is a possible solution, \\(\\lambda (w^*,b^*,\\delta^*)\\) for \\(\\lambda&gt;0\\) will be a solution too. Therefore, we normalize \\(\\delta=1\\). \\[min_{\\{w,b,\\delta\\}}\\ \\frac{1}{2}\\|w\\|^2\\] \\[s.t.\\ (w^Tx_i-b)y_i-1\\geq 0\\] Then do the Lagrange Multiplier. Kernel Function The whole discusstion above is about linear data, if not, we should mapping data to a higher dimension by Kernel Function. \\[x_i^Tx_j\\rightarrow\\emptyset(x_i)^T\\emptyset(x_j)\\] The kernel function could be very complicated, but by inner product, it will be simple a lot. \\[K(x_i,x_j)\\rightarrow\\emptyset(x_i)^T\\emptyset(x_j)\\] \\[K(x_i,x_j)\\rightarrow\\exp(-\\frac{\\|x_i-x_j||^2}{2\\sigma^2})\\] The Non-Separable case In real world, it is hard to find a optimal separating hyperplane(OSH), we need to insert a error term to handle the case that support vectors overlap with SupHs. \\[w^Tx_i-b\\leq-1+\\xi_i\\ \\forall y_i=-1\\] \\[w^Tx_i-b\\geq+1-\\xi_i\\ \\forall y_i=+1\\] \\[\\xi_i\\geq0\\] and made a cost function(“c” is the weight of cost): \\[cost = c(\\sum_i\\xi_i)^k\\] then \\[min\\ \\frac{1}{2}\\|w\\|^2+c(\\sum_i\\xi_i)^k\\] \\[s.t.\\ (w^Tx_i-b)y_i-1+\\xi_i\\geq 0\\] R-code example Cross-Validation &amp; Pre processing as above Tuning parameters the tuneLength is the number of errors allowed results_svm = train(quality~., data=wine_train, method=&#39;svmLinear2&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneLength=5, probability=TRUE) # to get probs along with classifications results_svm preds_svm = predict(results_svm, wine_test) confusionMatrix(preds_svm, good_observed, positive=&#39;good&#39;) Reference：http://www.cmlab.csie.ntu.edu.tw/~cyy/learning/tutorials/SVM2.pdf "],
["basic-on-unsupervised-learning.html", "Chapter 9 Basic on Unsupervised-Learning Cluster Latent Variable Models Graphical Structure Imputation", " Chapter 9 Basic on Unsupervised-Learning 這個章節想要回答的是，非監督式學習他能幫助我們解決什麼問題？常見的使用場合是？ 其目的是去對原始資料進行分類，以便了解資料內部結構。有別於監督式學習網絡，無監督式學習網絡在學習時並不知道其分類結果是否正確，亦即沒有受到監督式增強(告訴它何種學習是正確的)。其特點是僅對此種網絡提供輸入範例。而它會自己主動從這些範例中找出其潛在類別規則。當學習完成並經測試後，也能夠將之應用到新的案例上。 監督式學習：資料已有標記，運用已標記資料來做訓練，所以模型評估講求準度。 非監督式學習：資料沒有標記，從中找出擁有相同特徵的資料群，所以是找出資料間大致分布的趨勢，而沒有要預測的對象。 Cluster 分群是一種將資料分類成群的方法，為一種非監督式學習，也就是訓練資料沒有預先定義的標籤。其主要的目的在於找出資料中相似的幾個群聚，讓在同一個子集中的成員對象都有相似的一些屬性，常見的包括在坐標系中更加短的空間距離等。 一般而言，分群法可以大致歸為兩大類： 階層式分群法 (hierarchical clustering) : 群的數目可以由大變小(divisive hierarchical clustering)，或是由小變大(agglomerative hierarchical clustering)，來進群聚的合併或分裂，最後再選取最佳的群的數目。 分割式分群法 (partitional clustering) : 先指定群的數目後，再用一套疊代的數學運算法，找出最佳的分群方式以及相關的群中心。 常見的分法有K-平均演算法、階層式分群、混合模型。 K-平均演算法 參考資料： https://zh.wikipedia.org/wiki/K-平均算法 概念 把n個點劃分到k個聚類中，使得每個點都屬於離他最近的均值(聚類中心)對應的聚類，以之作為聚類的標準。 這個問題在計算上是 NP困難，不過存在高效的 啟發式演算法。一般情況下，都使用效率比較高的啟發式演算法，它們能夠快速收斂於一個局部最優解。 這些演算法通常類似於通過疊代最佳化方法處理高斯混合分布的最大期望演算法（EM演算法）。而且，它們都使用聚類中心來為資料建模；然而k-平均聚類傾向於在可比較的空間範圍內尋找聚類，期望-最大化技術卻允許聚類有不同的形狀。 聚類數目k是一個輸入參數，通常要進行特徵檢查以決定聚類數目。 演算法描述 已知觀測集(x1,x2,…,xn)，其中每個觀測都是一個d-維實向量，k-平均聚類要把這n個觀測劃分到k個集合中(k≤n),使得組內平方和（WCSS within-cluster sum of squares）最小。 Min WCSS : \\(\\sum\\ ^k _{i=1} \\sum\\ _{x\\in S _i} \\|x-\\mu_i \\|^2\\) 步驟一：分配(Assignment) 首先，我們先對資料隨機取了k個資料點，當作一開始的中心，每一個資料點會去衡量他與這k個資料點的距離，並與最近者同群。此時即可達到第一次分群，使組內平方和(WCSS)達最小。 步驟二：更新(Update) 計算上步得到的每一個分群的觀測值中心，作為新均值點。需要留意的是，這個觀測值中心不見得恰好真的存在樣本。 找到新的分群均值點之後再重新計算每一個樣本點與新的分群均值點的距離，有些樣本點會因此變換所屬群集。 交替進行的兩個步驟都會減小目標函式的值，並且分配方案（k個群集）只有有限種，所以演算法一定會收斂於某一（局部）最優解。 簡易圖文敘述 優缺點 優點： 簡單好理解，目標函數極小化容易操作。 項目自動分配給群集。 缺點： 收斂到局部最佳解，而不是整體最佳解。 聚類數目k是一個輸入參數，選擇不恰當的k值可能會導致糟糕的聚類結果，所以必須先預測集群數量（要檢查特徵）。 所有項目強制列入群集。 對極端值很敏感，但可用K-medians(中位數)、K-medoids(用真實資料點而非不一定存在的群集中心點）替代解決。 4.1 面對極端值時候，表示整個群集的分配並不是對稱鐘型，因此由組內平方和最小的目標函數求得的群集中心點並不能夠代表組內資料的中心，反而若用中位數或者特定的真實資料點可以解決這個問題。 4.2 進一步值得注意的是，用特徵分群時建議檢查特徵資料分配狀況並做適當的轉換（像是log轉換，pre-processing變得重要）來達到分配的對稱性。否則，若有些特徵具有偏態有些則否，會讓K-means的分類效果變得不好。 Latent Variable Models Thinking about Latent Variables by Michael Clark Latent Variable是一個很大的主題。多半是和降維有關，我如何用較少的變數來捕捉我想要觀察的東西，而且那個東西有時並不直觀，很多外顯的東西（indicator ，指標或者訊號）能夠代表他，但始終無法代表他的全部。 舉例來說，你如何衡量一個人有多快樂？快樂指數即可理解為一個latent variable 他們是否在笑 他們跟他人互動的情況如何 常見的分析手法 PCA 主成份分析 Factor Analysis 背後關鍵的處理原則 Dimension Reduction/Data compression : 我如何盡可能用較少的變數以及觀測值並保持住我所想要資料特性（資料變異性） Matrix Factorization: 我如何將一個大的矩陣，拆解成許多的小矩陣。 Latent Linear Models: 藉此彌補一個基本常用的Multi-OLS的不足。 PCA示範 we seek to find orthogonal (i.e. uncorrelated) factors to maximize the variance accounted for by each factor. 簡潔的PCA線性代數介紹 PCA主成分的重點是要降維，同時保留原始資料最大的變異性。 在機器學習上，若不提高維度則無法提升準確性，但維度提高到一定程度後對準確性的加分作用不大反而徒增困擾，因此我們需要降維。 數學上，我們其實是將一個很大的矩陣，舉例一個政治傾向調查結果，拆解小矩陣相乘+誤差項，過程我們稱之為factor loading。 其中小矩陣的rank(e.g. 3by3)，就代表你用了三個主成份來嘗試捕捉原本的矩陣，但是因為不會是100%，因此也是需要加上誤差項的原因。 我可以透過trial and error的方式來嘗試知道哪一個主成分邊際代表的資料變異量較高（或較低），以及搭配應用上你想要達到多少的比例（e.g.原始資料的80%資料變異量），最後來決定你需要保留幾個主成分。 主成分分析，有時候詮釋上並不容易，有時我們會稱為xxx相關特徵，但降維仍然是使用的第一目的。 #我們透過五大面向的問題（各五題）來衡量一個人的人格特質。 #Agreeableness #Conscientiousness #Openness to experience #Extraversion #Neuroticism library(psych) bfi bfi_trim = bfi %&gt;% select(matches(&#39;A[1-5]|N[1-5]&#39;)) #Agreeableness and Neuroticism items were chosen pc = principal(bfi_trim, 2) pc 重要的報表內容： SS loadings: 就是小矩陣的eigenvalue，也就是資料的變異量 Proportion Var: 2.9/10 = 0.29，10是指當初做主成份分析原始的10個變數，總變異量就是10。 Cumulative Var：累積資料變異量 h2：原本有A1~5 N1~5共10個變量。那最終降為到兩個主成份，請問這兩個主成份能夠解釋多少原本A1這個變數資料的變異性。 Loading，就是指那個主成份分析2 by 2的那個矩陣。又稱為pattern matrix，是兩個(10 by 2 matrix)轉置相乘。 Graphical Structure 非監督式學習用意是在找尋資料本身的大致的趨勢（pattern）或架構（structure），有些手法能夠用視覺化的方式（e.g.network analysis）來看到樣本間的關聯，包含誰跟誰較為相近等。 進一步可以參考：Graphical &amp; Latent Variable Modeling by Michael clark Imputation 當資料本身有缺失值的時候，也可以用非監督式學習的方式來計算推論可能的值。 舉例來說，推薦系統即是用既有的用戶資料，以及與該用戶相近的其他使用者的購買狀況進行推論它可能會喜歡什麼。 "],
["ensembles.html", "Chapter 10 Ensembles", " Chapter 10 Ensembles 實用連結： Ensemble Learning—Bagging and Boosting Boosting, Bagging, and Stacking—Ensemble Methods with sklearn and mlens 機器學習: Ensemble learning之Bagging、Boosting和AdaBoost 這跟非監督式學習還有關係嗎？ Ensembles中文稱作集成學習，意思是想辦法將多個預測模型合起來，然後預測效果（或分類）的表現可以勝過單一個模型。 一個比較傳神的比喻是： 每一個個別的機器學習模型都像是盲人摸象故事裡的盲人 若能將盲人（個別模型）所看到的結果綜合起來，我就可以得知大象是長怎麼樣子（有良好的預測或分類表現） 常見的手法即有以下三種：Bagging/Boosting/Stacking Bagging Bagging基本上就是Bootstrap aggregation。我透過不斷重新抽樣本並丟入模型（我們可以有多個模型並對其做訓練），透過反覆重抽，我可以觀察到資料本身的bias(the mean) &amp; variance(the standard deviation) 最終取所有模型的預測表現之平均。藉此可以在不影響偏誤（bias）下，降低我預測結果的變異程度（variance）。 至少我不會一次丟到靶心，下一次卻什麼也沒射中。 Boosting 是一種多模型的接力使用。 我會刻意放大A模型預測錯誤的地方之權重，並且做為接續之B模型的訓練重點。 R Code Example #The following object is masked from ‘package:dplyr’:slice #be aware the same slice function from two different package library(xgboost) modelLookup(&quot;xgbLinear&quot;) modelLookup(&quot;xgbTree&quot;) xgb_opts = expand.grid(eta=c(.3,.4), max_depth=c(9, 12), colsample_bytree=c(.6,.8), subsample=c(.5,.75,1), nrounds=1000, min_child_weight=1, gamma=0) set.seed(1234) #the R code form m-clark has the object called &#39;good&#39;. Be aware the variable name is different or not, and the technique are all the same results_xgb = train(quality~., data=wine_train, method=&#39;xgbTree&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneGrid=xgb_opts) results_xgb preds_gb = predict(results_xgb, wine_test) confusionMatrix(preds_gb, good_observed, positive=&#39;Good&#39;) Stacking 在stacking裡面，重要的是那些單一預測模型之預測結果，都是最終meta-learner模型所需的訓練資料的來源。 一般來說，若我們手上已經嘗試過多個單一預測模型，則我可以選擇像bagging一樣，將表現平均（voting），一般平均或者加權平均都沒問題。或者選擇stacking 首先我利用不同的單一模型做訓練，並且將資料餵入並得到每一個模型下的y_i_hat，並且將這些y_i_hat，用為meta_learner model的資料來源。 等到meta_learner選定好之後，並用原始資料以及y_i_hat的資料一起去訓練meta_learner模型。 stacking的感覺其實有一點像NNet，把中間加了一層，但是不代表越多層準確度就越好。 "],
["deep-learning.html", "Chapter 11 Deep Learning", " Chapter 11 Deep Learning 因為較難訓練這類的模型，所以一個模型就會被用到許多種情況上，這時候數據具有「comparable」的特性很重要。 深度學習廣泛地運用在AI技術、臉部辨識、電腦視覺、語音辨識、自然語言處理（NLP）…。常用的技術包含前饋神經網路（Feed Forward Network）、卷積神經網路（Convolutional Neural Network）、循環神經網絡（Recurrent Neural Networks）、結構遞歸神經網絡（Recursive Neural Networks），經過前幾章已經了解了基本的神經網路外，下一步便是可以去學習以上這些神經網路。 Python套件：tensorflow、pytorch、keras。 R套件：sparklyr、keras。 此書並未深入介紹深度學習，而是另外推薦 深度學習的網站，作者只有將R及Python的小範例放在附件中。 "],
["summary.html", "Chapter 12 Summary 12.1 Feature Selection &amp; Importance 12.2 Natural Language Processing/Text Analysis 12.3 Bayesian Approaches 12.4 More Stuff 12.5 Cautionary Notes 12.6 Some Guidelines", " Chapter 12 Summary 12.1 Feature Selection &amp; Importance 前面在講這個主題時，介紹很多不同的方法去衡量各變數的代表性，進而決定它們是否重要。從某一方面來說，當我們在選擇資料集時，就已經是一個特徵選擇的過程，因為雖然我們會希望資料跟理想中的一樣完整，但事實上資料會因為用戶輸入習慣、隱私問題、時間限制…，各種因素使得資料有所限制。然而針對不同的資料需要有不同的處理模型。 利用lasso係數的計算，非常不重要的變數最後權重可能為零，所以特徵選擇在模型預測非常重要。可以考慮各種方法，只要一個變數被排除時，預測的精準度明顯下降，就代表它很重要。 以往時常仰賴統計分析的顯著性去決定變數是否重要，但忽略對預測影響。在機器學習的技術上，我們將更多重點轉移到預測這件事。所以對於沒有經驗的人來說，可能需要新的方式來思考如何衡量重要性。 12.2 Natural Language Processing/Text Analysis 有些時候有興趣的數據非數字矩陣類型，而是文本的形式，做這類型的分析且文本資料很少時，大部分會花時間在資料整理上，這類數據的目的包含發現隱藏的主題、詞語標註、情感分析、語言辨識、運用字詞預測結果、檢查語句結構並使用網絡圖形呈現。機器學習也可運用在聲音，做語音辨識，深度學習更是廣泛。用R做的一些基本文字分析可以參考 這個網站。 12.3 Bayesian Approaches 雖然本書大部分是以傳統的措辭去敘述，但要注意許多概念與技術都可以延伸到貝氏的觀念，甚至有些機器學習的技術就是在貝氏的邏輯下運作，像是線上學習（online learning）。然而貝氏估計的核心概念卻很難應用在大規模的資料上。 12.4 More Stuff 機器學習還包含半監督式學習（semi-supervised learning）、線上學習（online learning）… 12.5 Cautionary Notes 所有的機器學習模型都有假設，當假設不成立時它原先的結果可能就有問題。或是有可能模型Ａ在學習上勝於模型Ｂ，但模型Ｂ在有限的樣本下表現比模型Ａ好。在沒有任何前提下，不會有哪個模型一定比較好的情況，也不是越複雜的模型就比較好。正確的作法應該是了解資料特性，找到適合的模型，最後再透過交叉驗證進一步調整模型，讓模型進步。 12.6 Some Guidelines 在做分析時可以先畫一個流程圖，以確保分析時有朝向設定的目標。 將選擇、調整特徵參數和最後的模型測試結果分開，避免評估錯誤。 學習多個模型後，再選擇或是組合出一個最佳的模型。 使用機器學習時，應該從最簡單的方法開始用起，再逐漸使用需要更多調整的模型。 Example：regularized logistic regression -&gt; random forest -&gt; your-fancy-technique. 避免過度配適。 More data beats a cleverer algorithm, but a lot of data is not enough by itself. Let the data speak for itself. “Nothing is more practical than a good theory.” "],
["offical-tutorial.html", "A Offical tutorial", " A Offical tutorial bookdown repo bookdown webpage For your convenience, you may open the minimal bookdown template in new project in RStudio whenever you want to write down a book. "],
["the-dataset.html", "B The Dataset", " B The Dataset wine &lt;- read.csv(&quot;~/Dropbox/M-Team/ML/wine.csv&quot;) str(wine) set.seed(1234) # so that the indices will be the same when re-run # 抽出80%樣本來train, output format is matrix trainIndices = createDataPartition(wine$quality, p=.8, list=F) # delete highly correlated free.sulfur and density wine_train = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% dplyr::slice(trainIndices) wine_test = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% dplyr::slice(-trainIndices) "],
["python-on-rstudio.html", "C Python on RStudio", " C Python on RStudio library(reticulate) conda_create(&quot;m-team-machine-learning&quot;) 找到Rprofile.site的R根目錄 site_path = R.home(component = &quot;home&quot;) fname = file.path(site_path, &quot;etc&quot;, &quot;Rprofile.site&quot;) file.exists(fname) file.edit(fname) #file.edit(&quot;/Library/Frameworks/R.framework/Resources/Rprofile.site&quot;) 未來開python環境用法 library(reticulate) use_python(Sys.getenv(&quot;condaPythonPath&quot;), required = T) use_condaenv(&quot;m-team-machine-learning&quot;) Google API Installation https://developers.google.com/api-client-library/python/start/installation conda_install(envname = &quot;m-team-machine-learning&quot;, c(&quot;numpy&quot;,&quot;pandas&quot;)) conda_install(envname = &quot;m-team-machine-learning&quot;, &quot;keras&quot;) conda_install(envname=&quot;m-team-machine-learning&quot;, c(&quot;google-api-python-client&quot;, &quot;google-auth-httplib2&quot;, &quot;google-auth-oauthlib&quot;), pip=TRUE) "],
["ml-vv.html", "D ML_VV", " D ML_VV wine &lt;- read.csv(&quot;~/Dropbox/M-Team/ML/wine.csv&quot;) str(wine) library(caret) library(tidyverse) library(glmnet) library(class) library(randomForest) library(e1071) library(ggplot2) Regularized Regression set.seed(1234) # so that the indices will be the same when re-run trainIndices = createDataPartition(wine$quality, p=.8, list=F) wine_train = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(trainIndices) wine_test = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(-trainIndices) wine_trainplot = select(wine_train, -quality) %&gt;% preProcess(method=&#39;range&#39;) %&gt;% #標準化處理range =&gt; (x-min)/(max-min) predict(newdata= select(wine_train, -quality)) #利用predict函數顯示出處理好的矩陣 featurePlot(wine_trainplot, wine_train$quality, &#39;box&#39;) cv_opts = trainControl(method=&#39;cv&#39;, number=10) #定義模型訓練參數，劃分十組交叉驗證（使用repeatedcv可重複劃分） regreg_opts = expand.grid(.alpha = seq(.1, 1, length = 5), .lambda = seq(.1, .5, length = 5)) #25種組合(決定lamda重要度？) results_regreg = train(quality~., data=wine_train, method = &quot;glmnet&quot;, trControl = cv_opts, preProcess = c(&quot;center&quot;, &quot;scale&quot;), #指定數據標準化，&quot;center&quot;和&quot;scale&quot;。其中center表示預測變量減去均值 tuneGrid = regreg_opts) results_regreg #kappa是一統計量指標衡量預測值與實質的差距 ggplot(results_regreg) preds_regreg = predict(results_regreg, wine_test) good_observed = wine_test$quality confusionMatrix(preds_regreg, good_observed, positive=&#39;good&#39;) The lower bound (and p-value) suggests we are statistically predicting better than the No Information Rate (i.e., just guessing the more prevalent ‘Bad’ category) -&gt; 猜好的比猜壞的還強 confusionMatrix(preds_regreg, good_observed, positive=&#39;good&#39;, mode=&#39;prec_recall&#39;) k-nearest Neighbors knn_opts = data.frame(k=c(seq(3, 11, 2), 25, 51, 101)) knn_opts results_knn = train(quality~., data=wine_train, method=&#39;knn&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneGrid = knn_opts) results_knn preds_knn = predict(results_knn, wine_test) confusionMatrix(preds_knn, good_observed, positive=&#39;good&#39;) Neural networks results_nnet = train(quality~., data=wine_train, method=&#39;avNNet&#39;, trControl=cv_opts, preProcess=c(&#39;center&#39;, &#39;scale&#39;), tuneLength=3, trace=F, maxit=3) results_nnet ggplot(results_nnet) preds_nnet = predict(results_nnet, wine_test) confusionMatrix(preds_nnet, good_observed, positive=&#39;good&#39;) Support Vector Machines 參考資料：https://goo.gl/cMRJLA 主要概念：SVM把原始資料投影到更高維度的空間裡，使得原本在低維度找不到切分的點，變成在高維度能夠找到一個超平面(hyperplane)，去切分。 而這個分界線會尋找最好的（距離兩分類最遠，也就是最明顯劃分）那個維度去切。 kernel function 參考資料：https://goo.gl/APV7NP example results_svm = train(quality~., data=wine_train, method=&#39;svmLinear2&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneLength=5, probability=TRUE) # to get probs along with classifications results_svm preds_svm = predict(results_svm, wine_test) confusionMatrix(preds_svm, good_observed, positive=&#39;good&#39;) 優點： 1.各式樣的資料皆能有不錯的預測表現 2.可以將變數做線性組合增強預測能力 缺點： 黑箱作業（不知道其中到底使用了什麼樣的映射函數） "]
]
