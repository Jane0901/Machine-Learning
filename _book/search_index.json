[
["k-nearest-neighbors.html", "Chapter 2 k-nearest Neighbors 概念 2.1 Strengths &amp; Weaknesses", " Chapter 2 k-nearest Neighbors 資料處理 載入wine.csv資料，並切分成訓練資料及測試資料 wine &lt;- read.csv(&quot;~/Dropbox/M-Team/ML/wine.csv&quot;) set.seed(1234) # so that the indices will be the same when re-run trainIndices = createDataPartition(wine$quality, p=.8, list=F) wine_train = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(trainIndices) wine_test = wine %&gt;% select(-free.sulfur.dioxide, -density, -color, -white) %&gt;% slice(-trainIndices) 概念 在分類酒品質好壞的例子中，藉由把其他變數以向量空間形式表示，針對欲分類的測試值Ａ，找出K個距離它最近的值，如果K個值中，多數為“好品質”，則我們也就假設Ａ也是“好品質”。 此方法在補缺失值（類別）時是個不錯的方法。 參考資料 # cross validation 10 cv_opts = trainControl(method=&#39;cv&#39;, number=10) #定義模型訓練參數，劃分十組交叉驗證（使用repeatedcv可重複劃分 knn_opts = data.frame(k=c(seq(3, 11, 2), 25, 51, 101)) knn_opts ## k ## 1 3 ## 2 5 ## 3 7 ## 4 9 ## 5 11 ## 6 25 ## 7 51 ## 8 101 results_knn = train(quality~., data=wine_train, method=&#39;knn&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), #指定數據標準化，&quot;center&quot;和&quot;scale&quot;。其中center表示預測變量減去均值 trControl=cv_opts, tuneGrid = knn_opts) results_knn ## k-Nearest Neighbors ## ## 5199 samples ## 10 predictor ## 2 classes: &#39;bad&#39;, &#39;good&#39; ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 4679, 4680, 4679, 4679, 4679, 4679, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 3 0.7599480 0.4764501 ## 5 0.7597501 0.4703899 ## 7 0.7595604 0.4697444 ## 9 0.7541728 0.4557870 ## 11 0.7551391 0.4564198 ## 25 0.7487904 0.4347320 ## 51 0.7466758 0.4258642 ## 101 0.7401380 0.4044246 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 3. preds_knn = predict(results_knn, wine_test) good_observed = wine_test$quality confusionMatrix(preds_knn, good_observed, positive=&#39;good&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction bad good ## bad 296 147 ## good 180 675 ## ## Accuracy : 0.7481 ## 95% CI : (0.7235, 0.7715) ## No Information Rate : 0.6333 ## P-Value [Acc &gt; NIR] : &lt; 2e-16 ## ## Kappa : 0.4496 ## Mcnemar&#39;s Test P-Value : 0.07679 ## ## Sensitivity : 0.8212 ## Specificity : 0.6218 ## Pos Pred Value : 0.7895 ## Neg Pred Value : 0.6682 ## Prevalence : 0.6333 ## Detection Rate : 0.5200 ## Detection Prevalence : 0.6587 ## Balanced Accuracy : 0.7215 ## ## &#39;Positive&#39; Class : good ## 2.1 Strengths &amp; Weaknesses Strengths 直覺的方法 對預測變量的離群值很強 Weaknesses 易受到不相干的特徵影響 能夠處理混和類型的資料 要預測的目標種類最好數量是差不多 資料大的情況下時間複雜度較高 雖然直覺好用，但根據上面的缺點，他的預測準確率是很差的，如果是機器學習初學者可以用，但隨著學的越多會發現有更多比這個好用的預測模型。 "]
]
