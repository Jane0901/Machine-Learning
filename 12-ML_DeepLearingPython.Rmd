---
title: "Python_DeepLearning_ch3"
author: "PoMingChen"
date: "3/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(reticulate)
use_python("/Users/chenpoming/anaconda3/envs/m-team2/bin/python", required = T)
use_condaenv("m-team2")
```

在R chunk內叫出Python code，用`py$`。反過來在python chunk叫出R code，用`r.`

```{python}
# from keras import models
# from keras import layers
# model = models.Sequential()
# model.add(layers.Dense(32, input_shape=(784,)))
# model.add(layers.Dense(32))
# 
# print(model)
```

#### Layers

layer -> layers to be a model -> Loss function and Optimizer(under SGD)

### Keras

1. Keras and TensorFlow 差在哪邊？

2. Developing overview

- Define your training data

- Define a network of layers (or model)

- Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.

- Iterate on your training data by calling the `fit()` method of your model

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(784,)))
model.add(layers.Dense(10, activation='softmax'))
```

same model as above defined using functionl API

**何謂functional API? (ON CHAPTER 7**
```{python}
# input_tensor = layers.Input(shape=(784,))
# x = layers.Dense(32, activation='relu')(input_tensor)
# output_tensor = layers.Dense(10, activation='softmax')(x)
# model = models.Model(inputs=input_tensor, outputs=output_tensor)
# 
# from keras import optimizers
# model.compile(optimizer=optimizers.RMSprop(lr=0.001),loss='mse',metrics=['accuracy'])
# # model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)
# #input_tensor就是input data
# #target_tensor就是output data的概念
```

### set up deep-learning workstation

> If you’re seriousabout deep learning, you should set up a local workstation with one or more GPUs

## 3.4 Classifying movie reviews: a binary classification example

這不是子婷當初寒假跟我聊到的case！？

```{python}
from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
#The argument num_words=10000 means you’ll only keep the top 10,000 most fre-quently occurring words in the training data. Rare words will be discarded. This allowsyou to work with vector data of manageable size.
```

```{python}
train_data[0]
train_labels[0]


#where 0 stands for negative and 1 stands for positive:
```

```{r}
#放到R裡面去比較好看
py$train_data -> train_data
py$train_labels -> train_labels
```

```{python}
word_index = imdb.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
```

Move on to 3.4.2 Prepare the data(處理資料)

> Python的list，相當是R的vector

> One-hot encode

#### Listing 3.2
```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

```{python}
x_train[0]
```

```{python}
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32') #float32，有32/64，看他佔記憶體的大小。

y_test
y_train
```

#### Listing 3.3 The Model Definition(the whole model)

> 激活函數activation functions，負責為神經網路引入非線性特徵。看到不知道那個函數分配的話，就是google一下看看。

What are activation functions, and why are they necessary?

Without an activation function like `relu` (also called a non-linearity), the `Dense` layerwould consist of two linear operations—a dot product and an addition:

`output = dot(W, input) + b`

So the layer could only learn linear transformations (affine transformations) of theinput data: the hypothesis space of the layer would be the set of all possible lineartransformations of the input data into a 16-dimensional space.

Such a hypothesisspace is too restricted and wouldn’t benefit from multiple layers of representations,because a deep stack of linear layers would still implement a linear operation: addingmore layers wouldn’t extend the hypothesis space.

In order to get access to a much richer hypothesis space that would benefit fromdeep representations, you need a non-linearity, or activation function. `relu` is themost popular activation function in deep learning, but there are many other candi-dates, which all come with similarly strange names: `prelu`, `elu`, and so on

```{python}
from keras import models
from keras import layers

model = models.Sequential() #另一個方式是functional api
model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) #第一層
model.add(layers.Dense(16, activation='relu')) #第二層
model.add(layers.Dense(1, activation='sigmoid')) #第三層（結果輸出）（1or0）（a probalility to be 1 or 0)

#從第一層第二層，就有點類似PCA，壓縮到只剩下十六個變數等。第二層仍然是16個變數等。
```

#### Listing 3.4 Compelling the model(目前complie只有翻譯給電腦看，但是還沒有做任何的運算)
```{python}
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
```

#### Listing 3.5 Configuring the optimizer
```{python}
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy']) #metrics是用來兩個東西然後衡量彼此的距離
```

#### Listing 3.6 Using the custom losses and metrics
```{python}
from keras import losses
from keras import metrics
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
              loss=losses.binary_crossentropy,
              metrics=[metrics.binary_accuracy])
```

#### Listing 3.7 Setting aside a validation set
```{python}
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```

#### Listing 3.8 Training the model
```{python}
from keras import losses
from keras import metrics
from keras import optimizers
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
              
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20, #數值分析連續跑20次
                    batch_size=512,
                    validation_data=(x_val, y_val))
#後面會再介紹cross validation
```

```{python}
import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(history_dict['acc']) + 1)
plt.plot(epochs, loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


```{python}
plt.clf()
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']
plt.plot(epochs, acc_values, 'bo', label='Training acc')
plt.plot(epochs, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

#### Listing 3.11 Retraining the model from scratch
```{python}
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
              
model.fit(x_train, y_train, epochs=4, batch_size=512) #數值分析連續跑四次
results = model.evaluate(x_test, y_test)
```

```{python}
model.predict(x_test)
```

## 3.5 Classifying newswires: a multiclass classification example

> 3.5 和3.4滿像

```{python}
from keras.datasets import reuters
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
```

```{python}
len(train_data)
len(test_data)
train_data[10]
train_labels[10]
```

#### Listing 3.14 (preparing the data)

```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000): 
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results
  
#def是define的意思，就是定義一個函數。
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

```{python}
from keras.utils.np_utils import to_categorical
one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
```

```{python}
from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
#You end the network with a Dense layer of size 46. This means for each inputsample, the network will output a 46-dimensional vector
#the softmax, It means the network will output a probability distribution over the 46 different output classes.
#for every input sample, the network will produce a 46-dimensional output vector, where output[i] is the probability that the samplebelongs to class i. The 46 scores will sum to 1
#舉例：一篇新聞進去，出來他會分類出，他屬於第一類政治的機率0.13，第二類體育0.08，依此類推，這46個項目加總是1，你可以從機率分配排序上知道他可能是哪一類的新聞。
```

model compiling
```{python}
model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])
```

set apart 1000 sample as validation set
```{python}
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
```

train the model(with Listing 3.18)
```{python}
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

plot the result (training and validation loss)(Listing 3.19)
```{python}
import matplotlib.pyplot as plt
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

plot the result (training and validation accuracy)(Listing 3.19)
```{python}
plt.clf()
acc = history.history['acc']
val_acc = history.history['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

#### Listing 3.21 Retraining the model from scratch

> After we do the training and validtion set, we find epochs = 9 is the best. (or we will get overfitted) 

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(partial_x_train,
          partial_y_train,
          epochs=9,
          batch_size=512,
          validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)
results

#0.7858414960459524 we tune the training set to its best performance, and let's see what will happen in new data.
```

#### Listing 3.22 Generating predictions for new data

> 看看預測資料丟進去，他會預測這則新聞屬於哪一類的新聞。

```{python}
predictions = model.predict(x_test)
predictions[0].shape #第一筆資料丟進去，他出來的是一個長度46的vector，表示對每一個新聞類別的機率分配是多少。
np.sum(predictions[0]) #這46個新聞類別的機率總和是1
np.argmax(predictions[0]) #他最有可能屬於哪一種類型的新聞
```



> 注意model.train & model.evaluate的差別。

> 務必搞清楚one-hot coding概念

> 建議參考Keras的cheatsheet，Define compile fit evaulate predict，中間三個步驟用Python，其他都回到R，做資料處理。

> `善用在R Code codechunk py$predictions %>% class`


#### 3.5.6 A different way to handle the labels and the loss

The only thing this approach would change is the choice of the loss function. The lossfunction used in listing 3.21, categorical_crossentropy, expects the labels to followa categorical encoding. With integer labels, you should use `sparse_categorical_crossentropy`:

```{python}
model.compile(optimizer='rmsprop',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
```

#### 3.23 A model with an information bottleneck

> 因為最終要變成一個46類別的機率分配，若是中間有任何一層的layer，他擁有的dimension(unit)太低，則反而會濾掉太多資訊，且不可逆。因此不要讓任何一層的dimension遠低於你最終的output的dimension

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])
model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=128,validation_data=(x_val, y_val))

#重點是那個4，因為你在某一層壓縮到只剩下4個元素。這樣要在放到46個類別。所以就比較吃力。（注意一下要先執行完畢前面的程式）
```

## 3.6 Predicting house prices: a regression example

the Boston pricing dataset

```{python}
from keras.datasets import boston_housing
(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()
```


```{python}
python
```


