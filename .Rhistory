select(-X,-free.sulfur.dioxide, -density,  -color, -white) %>%
slice(-trainIndices)
wine_trainplot = select(wine_train, -quality) %>%
preProcess(method='range') %>%
predict(newdata= select(wine_train, -quality))
good_observed = wine_test$quality
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
results_nnet
ggplot(results_nnet)
ggplot(results_nnet) +
labs(x='Number of Hidden Units') +
scale_x_continuous(breaks = c(1,3,5,7,9))
preds_nnet = predict(results_nnet, wine_test)
confusionMatrix(preds_nnet, good_observed, positive='Good')
library(doParallel)
cl = makeCluster(2)
registerDoParallel(cl)
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
results_nnet
ggplot(results_nnet)
ggplot(results_nnet) +
labs(x='Number of Hidden Units') +
scale_x_continuous(breaks = c(1,3,5,7,9))
preds_nnet = predict(results_nnet, wine_test)
confusionMatrix(preds_nnet, good_observed, positive='Good')
results_nnet
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
results_nnet
ggplot(results_nnet)
ggplot(results_nnet) +
labs(x='Number of Hidden Units') +
scale_x_continuous(breaks = c(1,3,5,7,9))
preds_nnet = predict(results_nnet, wine_test)
confusionMatrix(preds_nnet, good_observed, positive='Good')
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
confusionMatrix(preds_nnet, good_observed, positive='good') #作者是Good，我們是寫good
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=3,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=3,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=6,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
library(doParallel)
cl = makeCluster(2)
registerDoParallel(cl)
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=6,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
results_nnet
ggplot(results_nnet)
ggplot(results_nnet) +
labs(x='Number of Hidden Units') +
scale_x_continuous(breaks = c(1,3,5,7,9))
preds_nnet = predict(results_nnet, wine_test)
confusionMatrix(preds_nnet, good_observed, positive='good') #作者是Good，我們是寫good
#results_nnet1 = train(quality~.,
# data=wine_train,
#method='mlpWeightDecayML',
#trControl=cv_opts,
#preProcess=c('center', 'scale'),
#trace=F,
#maxit=10)
#results_nnet1
results_nnet
cv_opts = trainControl(method='cv', number=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=6,
preProcess=c('center', 'scale'),
trace=T,
maxit=10)
results_nnet
ggplot(results_nnet)
ggplot(results_nnet) +
labs(x='Number of Hidden Units') +
scale_x_continuous(breaks = c(1,3,5,7,9))
preds_nnet = predict(results_nnet, wine_test)
confusionMatrix(preds_nnet, good_observed, positive='good') #作者是Good，我們是寫good
#results_nnet1 = train(quality~.,
# data=wine_train,
#method='mlpWeightDecayML',
#trControl=cv_opts,
#preProcess=c('center', 'scale'),
#trace=F,
#maxit=10)
#results_nnet1
results_nnet
results_nnet
results_nnet
View(results_nnet)
ggplot(results_nnet) +
labs(x='Number of Hidden Units') +
scale_x_continuous(breaks = c(1,3,5,7,9))
results_nnet
plot_min_depth_interactions(results_rf$finalModel, k=7)
knitr::opts_chunk$set(echo = TRUE,eval=FALSE)
library(randomForestExplainer)
plot_min_depth_distribution(results_rf$finalModel)
rf_opts = data.frame(mtry=c(2:6))
results_rf = train(quality~.,
data = wine_train,
method = 'rf',
preProcess = c('center', 'scale'),
trControl = cv_opts,
tuneGrid = rf_opts,
localImp = T,
ntree=100)
library(doParallel)
cl = makeCluster(2)
registerDoParallel(cl)
library(gplots)
library(caret)
library(tidyverse)
wine <- read.csv('~/Dropbox/M-team/ML/wine.csv')
wine$quality=factor(wine$quality)
set.seed(1234) # so that the indices will be the same when re-run
trainIndices = createDataPartition(wine$quality, p=.8, list=F)
wine_train = wine %>%
select(-X,-free.sulfur.dioxide, -density, -color, -white) %>%
slice(trainIndices)
wine_test = wine %>%
select(-X,-free.sulfur.dioxide, -density,  -color, -white) %>%
slice(-trainIndices)
wine_trainplot = select(wine_train, -quality) %>%
preProcess(method='range') %>%
predict(newdata= select(wine_train, -quality))
good_observed = wine_test$quality
cv_opts = trainControl(method='cv', number=10) # cross-validation
rf_opts = data.frame(mtry=c(2:6))
results_rf = train(quality~.,
data = wine_train,
method = 'rf',
preProcess = c('center', 'scale'),
trControl = cv_opts,
tuneGrid = rf_opts,
localImp = T,
ntree=100)
results_rf
library(randomForestExplainer)
plot_min_depth_distribution(results_rf$finalModel)
?plot_min_depth_interactions
preds_rf = predict(results_rf, wine_test)
preds_rf
confusionMatrix(preds_rf, good_observed, positive='good')
confusionMatrix(preds_rf, good_observed, positive='good')
varImp(results_rf)
knitr::opts_chunk$set(echo = FALSE)
library(reticulate)
use_python("/anaconda3/bin/python3.6", required = T)
use_condaenv("/anaconda3/envs/m-team")
source_python("../programs/學分超修.py")
library(plyr)
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(Rmisc)
library(kableExtra)
View(dfclean)
save(dfclean)
save(dfclean,學生型態資料)
save(dfclean,學生型態資料.Rda)
save(dfclean,學生型態資料.rda)
save(dfclean,"學生型態資料.rda")
save(dfclean,"學生型態資料".rda)
save(dfclean,"學生型態資料.RData")
save(dfclean,file=file = "學生型態資料.RData")
save(dfclean,file= "學生型態資料.RData")
save(dfclean,file= "學生型態資料.RData")
save(dfclean,file= "學生型態資料.RData")
save(dfclean,file= "學生型態資料.RData")
View(dfpr)
knitr::opts_chunk$set(echo = TRUE,eval=FALSE)
library(doParallel)
cl = makeCluster(2)
registerDoParallel(cl)
stopCluster(cl)
library(gplots)
library(caret)
library(tidyverse)
wine <- read.csv('~/Dropbox/M-team/ML/wine.csv')
wine$quality=factor(wine$quality)
set.seed(1234) # so that the indices will be the same when re-run
trainIndices = createDataPartition(wine$quality, p=.8, list=F)
wine_train = wine %>%
select(-X,-free.sulfur.dioxide, -density, -color, -white) %>%
slice(trainIndices)
wine_test = wine %>%
select(-X,-free.sulfur.dioxide, -density,  -color, -white) %>%
slice(-trainIndices)
wine_trainplot = select(wine_train, -quality) %>%
preProcess(method='range') %>%
predict(newdata= select(wine_train, -quality))
good_observed = wine_test$quality
cv_opts = trainControl(method='cv', number=10) # cross-validation
library(randomForestExplainer)
plot_min_depth_distribution(results_rf$finalModel)
rf_opts = data.frame(mtry=c(2:6))
results_rf = train(quality~.,
data = wine_train,
method = 'rf',
preProcess = c('center', 'scale'),
trControl = cv_opts,
tuneGrid = rf_opts,
localImp = T,
ntree=10)
rf_opts = data.frame(mtry=c(2:6))
results_rf = train(quality~.,
data = wine_train,
method = 'rf',
preProcess = c('center', 'scale'),
trControl = cv_opts,
tuneGrid = rf_opts,
localImp = T,
ntree=100)
library(gplots)
library(caret)
library(tidyverse)
wine <- read.csv('~/Dropbox/M-team/ML/wine.csv')
wine$quality=factor(wine$quality)
set.seed(1234) # so that the indices will be the same when re-run
trainIndices = createDataPartition(wine$quality, p=.8, list=F)
wine_train = wine %>%
select(-X,-free.sulfur.dioxide, -density, -color, -white) %>%
slice(trainIndices)
wine_test = wine %>%
select(-X,-free.sulfur.dioxide, -density,  -color, -white) %>%
slice(-trainIndices)
wine_trainplot = select(wine_train, -quality) %>%
preProcess(method='range') %>%
predict(newdata= select(wine_train, -quality))
good_observed = wine_test$quality
cv_opts = trainControl(method='cv', number=10) # cross-validation
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
results_nnet = train(good~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
library(doParallel)
cl = makeCluster(2)
registerDoParallel(cl)
results_nnet = train(quality~.,
data=wine_train,
method='avNNet',
trControl=cv_opts,
tuneLength=5,
preProcess=c('center', 'scale'),
trace=F,
maxit=10)
rf_opts = data.frame(mtry=c(2:6))
results_rf = train(quality~.,
data = wine_train,
method = 'rf',
preProcess = c('center', 'scale'),
trControl = cv_opts,
tuneGrid = rf_opts,
localImp = T,
ntree=10)
rf_opts = data.frame(mtry=c(2:6))
results_rf = train(quality~.,
data = wine_train,
method = 'rf',
preProcess = c('center', 'scale'),
trControl = cv_opts,
tuneGrid = rf_opts,
localImp = T,
ntree=10)
results_rf
preds_rf = predict(results_rf, wine_test)
preds_rf
confusionMatrix(preds_rf, good_observed, positive='good')
varImp(results_rf)
library(randomForestExplainer)
plot_min_depth_distribution(results_rf$finalModel)
plot_min_depth_interactions(results_rf$finalModel, k=7)
# tibble causes problem so convert wine_train to standard df.
library(ggRandomForests)
rf2 = rfsrc(formula = quality ~.,
data = data.frame(wine_train),
mtry = results_rf$finalModel$mtry)
gg_v = gg_variable(rf2)
gg_md = gg_minimal_depth(rf2)
# We want the top two ranked minimal depth variables only
xvar = gg_md$topvars[1:2]
plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1)
plot_explanations(rf_explain)
results_rf$finalModel$mtry
View(results_rf)
gg_v = gg_variable(rf2)
gg_v
gg_md
View(gg_v)
gg_v
gg_md
plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1)
plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1)
plot_explanations(rf_explain)
$\sum{pi_{{x}_i}$
set.seed(1234)
sample_index = sample(1:nrow(wine_test), 5)
sample_test = wine_test %>%
slice(sample_index) %>%
select(-quality)
library(lime)
rf_lime = lime(wine_train, results_rf)
rf_explain = explain(sample_test,
rf_lime,
n_features = 3,
feature_select = 'highest_weights',
labels = 'Good')#error:y is constant; gaussian glmnet fails at standardization step
View(rf_lime)
# tibble causes problem so convert wine_train to standard df.
library(ggRandomForests)
rf2 = rfsrc(formula = quality ~.,
data = data.frame(wine_train),
mtry = results_rf$finalModel$mtry)
gg_v = gg_variable(rf2)
gg_md = gg_minimal_depth(rf2)
gg_v
gg_md
xvar = gg_md$topvars[1:2]
plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1)
plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1)
rf_explain = explain(sample_test,
rf_lime,
n_features = 3,
feature_select = 'highest_weights',
labels = 'good')#error:y is constant; gaussian glmnet fails at standardization step
knitr::opts_chunk$set(echo = TRUE,eval=FALSE)
library(doParallel)
cl = makeCluster(2)
registerDoParallel(cl)
#stopCluster(cl)關閉
library(gplots)
library(caret)
library(tidyverse)
wine <- read.csv('~/Dropbox/M-team/ML/wine.csv')
wine$quality=factor(wine$quality)
set.seed(1234) # so that the indices will be the same when re-run
trainIndices = createDataPartition(wine$quality, p=.8, list=F)
wine_train = wine %>%
select(-X,-free.sulfur.dioxide, -density, -color, -white) %>%
slice(trainIndices)
wine_test = wine %>%
select(-X,-free.sulfur.dioxide, -density,  -color, -white) %>%
slice(-trainIndices)
wine_trainplot = select(wine_train, -quality) %>%
preProcess(method='range') %>%
predict(newdata= select(wine_train, -quality))
good_observed = wine_test$quality
rf_opts = data.frame(mtry=c(2:6))
results_rf = train(quality~.,
data = wine_train,
method = 'rf',
preProcess = c('center', 'scale'),
trControl = cv_opts,
tuneGrid = rf_opts,
localImp = T,
ntree=10)
results_rf
set.seed(1234)
sample_index = sample(1:nrow(wine_test), 5)
sample_test = wine_test %>%
slice(sample_index) %>%
select(-quality)
library(lime)
rf_lime = lime(wine_train, results_rf)
rf_explain = explain(sample_test,
rf_lime,
n_features = 3,
feature_select = 'highest_weights',
labels = 'good')#error:y is constant; gaussian glmnet fails at standardization step
plot_features(rf_explain)
plot_explanations(rf_explain)
sample_index = sample(1:nrow(wine_test), 5)
sample_index
set.seed(1234)
sample_index = sample(1:nrow(wine_test), 6)
sample_index
sample_test = wine_test %>%
slice(sample_index) %>%
select(-quality)
library(lime)
rf_lime = lime(wine_train, results_rf)
rf_explain = explain(sample_test,
rf_lime,
n_features = 3,#只看三種特徵的組合
feature_select = 'highest_weights',
labels = 'good')
plot_features(rf_explain)
plot_explanations(rf_explain)
sample_test
rf_lime = lime(wine_train, results_rf)
rf_lime
rf_explain = explain(sample_test,
rf_lime,
n_features = 3,#只看三種特徵的組合
feature_select = 'highest_weights',
labels = 'good')
rf_explain
plot_features(rf_explain)
rf_explain#各case跑完lime的係數狀況
sample_test
View(sample_test)
rf_explain = explain(sample_test,
rf_lime,
n_features = 3,#只看三種特徵的組合
feature_select = 'highest_weights',
labels = 'good')
rf_explain#各case跑完lime的係數狀況
plot_features(rf_explain)
set.seed(1234)
sample_index = sample(1:nrow(wine_test), 5)#隨機選取幾個case
sample_test = wine_test %>%
slice(sample_index) %>%
select(-quality) #分別拿掉5個case的outcome
library(lime)
rf_lime = lime(wine_train, results_rf)#lime
rf_explain = explain(sample_test,
rf_lime,
n_features = 3,#只看三種特徵的組合
feature_select = 'highest_weights',
labels = 'good')
rf_explain#各case跑完lime的係數狀況
plot_features(rf_explain)
plot_explanations(rf_explain)
knitr::opts_chunk$set(echo = TRUE,eval=FALSE)
# tibble causes problem so convert wine_train to standard df.
library(ggRandomForests)
rf2 = rfsrc(formula = quality ~.,
data = data.frame(wine_train),
mtry = results_rf$finalModel$mtry)
