---
title: "Deep Learning"
author: "陳宜榛"
date: "3/19/2019"
output: html_document
---
# Environment setup{-}

```{r}
library(reticulate)
use_python("~/anaconda3/envs/m-team-machine-learning/bin/python",
           required = T)
use_condaenv("m-team-machine-learning")
```


```{r, eval=FALSE}
conda_install(envname="m-team-machine-learning",
              c("keras","tensorflow","pip"),
              pip=TRUE)
```

# CH3{-}

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(784,)))
model.add(layers.Dense(10, activation='softmax'))
```

```{python}
input_tensor = layers.Input(shape=(784,))
x = layers.Dense(32, activation='relu')(input_tensor)
output_tensor = layers.Dense(10, activation='softmax')(x)
model = models.Model(inputs=input_tensor, outputs=output_tensor)
```

```{python}
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss='mse',
metrics=['accuracy'])
```

# 3.4 二元分類範例（Classifying movie reviews:a binary classification example）{-}

根據電影評論的文字內容，分類正面或負面的評論。

## 引入資料（The IMDB dataset）{-}

來自電影評論網站的50,000個評論。

### Loading the IMDB dataset{-}

```{python}
from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
num_words=10000) # 在資料中留下前10,000個最常出現的詞
```

```{python}
# train_data and test_data are lists of reviews： each review is a list of word indices (encoding a sequence of words)
print(train_data[0])
```

```{r}
py$train_data->train_data
py$train_labels->train_labels # 1 正評； 0 負評
```

```{python}
# train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive
print(train_labels[0])
```

```{python}
# no word index will exceed 10,000
print(max([max(sequence) for sequence in train_data]))
```

```{python}
# how you can quickly decode one of these reviews back to English words?
word_index = imdb.get_word_index()
reverse_word_index = dict(
[(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join(
[reverse_word_index.get(i - 3, '?') for i in train_data[0]]) 
# the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
```

[Python Dictionary get() Method](https://www.w3schools.com/python/ref_dictionary_get.asp)

```{r}
library(dplyr)
py$word_index %>% View
py$reverse_word_index
```

## 處理資料（Preparing the data）{-}

### Encoding the integer sequences into a binary matrix{-}

Turn the lists into tensors:

- Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the Embedding layer, which we’ll cover in detail later in the book).

```{r}
py$train_data # this is a tensor example.
```

- **One-hot encode** your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a Dense layer, capable of handling floating-point vector data.

```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension)) # Creates an all-zero matrix of shape (len(sequences), dimension)
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1. # Sets specific indices of results[i] to 1s
  return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
```

```{python}
print(x_train[0])
```

## 建神經網絡（Building your network）{-}

### The model definition{-}
 
- How many layers to use?

Two intermediate layers with 16 hidden units each
 
- How many hidden units to choose for each layer?

A third layer that will output the scalar prediction regarding the sentiment of the current review

在第四章才會解釋如何決定

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu')) # Two intermediate layers with 16 hidden units each
model.add(layers.Dense(1, activation='sigmoid')) # A third layer that will output the scalar prediction regarding the sentiment of the current review
```

NOTE:激活函數（activation functions）負責爲神經網絡引入非線性特徵。

[relu activation function](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7)

### Compiling the model{-}

- loss funtion:也可選擇mean_squared_error，但在二元分類中，binary_crossentropy是較好的選擇。

Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.

```{python}
model.compile(optimizer='rmsprop',
loss='binary_crossentropy',
metrics=['accuracy'])
```

### Configuring the optimizer{-}

自行選擇optimizer的方式

```{python}
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss='binary_crossentropy',
metrics=['accuracy'])
```

### Using custom losses and metrics{-}

自行選擇metrics的方式

```{python}
from keras import optimizers
from keras import losses
from keras import metrics
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss=losses.binary_crossentropy,
metrics=[metrics.binary_accuracy])
```

```{r}
py$metrics %>% {.$binary_accuracy}
```

## 驗證（Validating your approach）{-}

### Setting aside a validation set{-}

```{python}
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```

```{r}
py$x_val %>% class # it's a matrix
py$x_val %>% dim
py$y_val %>% dim
py$x_val -> x_val
x_val[1,]
```


### Training your model{-}

```{python}
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

```{python}
help("model.fit()")
```

```{python}
history_dict = history.history
print(history_dict.keys())
```

### Plotting the training and validation loss{-}

```{r, eval=FALSE}
conda_install("m-team-machine-learning",
  packages = "matplotlib")
```

```{r}
py$history_dict %>% names
```

```{python}
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(1, len(history_dict['acc']) + 1)

plt.plot(epochs, loss_values, 'bo', label='Training loss') # bo:blue dot
plt.plot(epochs, val_loss_values, 'b', label='Validation loss') # b:solid blue line
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```

### Plotting the training and validation accuracy{-}

```{python}
plt.clf() # Clears the figure

acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']

plt.plot(epochs, acc_values, 'bo', label='Training acc')
plt.plot(epochs, val_acc_values, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```

### Retraining a model from scratch{-}

```{python}
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
loss='binary_crossentropy',
metrics=['accuracy'])
model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)
```

```{r, eval=FALSE}
py$x_train %>% class
py$x_train -> x_train
x_train[1,1:50]
```

```{python}
print(results)
```


## 預測（Using a trained network to generate predictions on new data）{-}

```{python}
print(model.predict(x_test))
```

## 可調參數 （Further experiments）{-}

- Try using one or three hidden layers, and see how doing so affects validation and test accuracy.

- Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.

- Try using the `mse` loss function instead of `binary_crossentropy`.

- Try using the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.

# 3.5 多元分類範例（Classifying newswires:a multiclass classification example）{-}

根據路透社在1986年出版的一些新聞，分類為46個主題。

## 引入資料（The Reuters dataset）{-}

8,982個訓練集和2,246個測試集。

###  Loading the Reuters dataset{-}

```{python}
from keras.datasets import reuters
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(
num_words=10000) #在資料中留下前10,000個最常出現的詞
```

```{python}
# 8,982 training examples
print(len(train_data))
```

```{python}
# 2,246 test examples
print(len(test_data))
```

```{python}
# each example is a list of integers (word indices)
print(train_data[10])
```

```{python}
# how you can decode it back to words?
word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in
  train_data[0]]) # the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
```

```{python}
print(train_labels[10])
```

## 處理資料（Preparing the data）{-}

### Encoding the data{-}

```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

### One-hot encoding{-}

獨熱編碼，又稱一位有效編碼，其方法是使用N位狀態暫存器來對N個狀態進行編碼，每個狀態都有它獨立的暫存器位，並且在任意時候，其中只有一位有效。

```{python}
# 自己寫的方法
def to_one_hot(labels, dimension=46):
  results = np.zeros((len(labels), dimension))
  for i, label in enumerate(labels):
    results[i, label] = 1.
  return results
one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)
```

```{python}
# keras內建的方法
from keras.utils.np_utils import to_categorical
one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
```

```{r}
py$one_hot_test_labels %>% class
py$one_hot_test_labels -> one_hot_test_labels
one_hot_test_labels %>% dim
one_hot_test_labels %>% head
```

## 建神經網絡（Building your network{-}

### Model definition{-}

- You end the network with a `Dense` layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.

- The last layer uses a `softmax` activation. It means the network will output a probability distribution over the 46 different output classes—for every input sample, the network will produce a 46-dimensional output vector, where output[i] is the probability that the sample belongs to class i. The 46 scores will sum to 1.

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax')) # softmax:the network will output a probability distribution over the 46 different output classes
```

### Compiling the model{-}

```{python}
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
```

## 驗證（Validating your approach）{-}

### Setting aside a validation set{-}

```{python}
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
```

### Training the model{-}

```{python}
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

### Plotting the training and validation loss{-}

```{python}
import matplotlib.pyplot as plt
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### Plotting the training and validation accuracy{-}

```{python}
plt.clf()
acc = history.history['acc']
val_acc = history.history['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### Retraining a model from scratch{-}

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(partial_x_train,
partial_y_train,
epochs=9,
batch_size=512,
validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)
```

```{python}
print(results)
```

```{python}
import copy
test_labels_copy = copy.copy(test_labels)
np.random.shuffle(test_labels_copy)
hits_array = np.array(test_labels) == np.array(test_labels_copy)
print(float(np.sum(hits_array)) / len(test_labels))
```

## 預測（Using a trained network to generate predictions on new data）{-}

```{python}
predictions = model.predict(x_test)
```

```{python}
# Each entry in predictions is a vector of length 46
print(predictions[0].shape)
```

```{r}
py$predictions %>% class
py$predictions -> predictions
predictions[1,] %>%
  {which(.==max(.))}
predictions[1,] %>% View
```

```{python}
# The coefficients in this vector sum to 1
print(np.sum(predictions[0]))
```

```{python}
# The largest entry is the predicted class—the class with the highest probability
print(np.argmax(predictions[0]))
```
從0開始數，所以3代表第四類

## 可調參數 （Further experiments）{-}

- Try using larger or smaller layers: 32 units, 128 units, and so on.

- Try using a single hidden layer, or three hidden layers.

## 其他課題{-}

### A different way to handle the labels and the loss{-}

```{python}
y_train = np.array(train_labels)
y_test = np.array(test_labels)
```

```{r}
py$y_train %>% class
py$y_train -> y_train
y_train %>% head
```

<https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/>

The only thing this approach would change is the choice of the loss function. So use sparse_categorical_crossentropy to deal with regular tensor labels.

```{python}
model.compile(optimizer='rmsprop',
loss='sparse_categorical_crossentropy',
metrics=['acc'])
```

### The importance of having sufficiently large intermediate layers{-}

drop掉太多內容，會導致準確率下降

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(partial_x_train,
partial_y_train,
epochs=20,
batch_size=128,
validation_data=(x_val, y_val))
```

# 3.6 迴歸範例（Predicting house prices: a regression example）{-}

根據1970年代中期波士頓郊區的相關數據，例如犯罪率、財產稅率等，預測房屋的價格中位數。

## 引入資料（The Boston Housing Price dataset）{-}

共506個資料點，分成404個訓練集和102個測試集。

###  Loading the Boston housing dataset{-}

```{python}
from keras.datasets import boston_housing
(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()
```

```{python}
print(train_data.shape)
print(test_data.shape)
```

```{python}
print(train_targets)
```

價格落在10,000到50,000之間，因為時間是1970年代中期，且沒調整過通膨，所以才如此便宜。

```{r}
py$train_data->train_data
py$train_targets->train_targets
```

共13個變數，其中的變數有人均犯罪率（per capita crime rate）、每棟住宅的平均房間數量（average number of rooms per dwelling）、高速公路的易達性（accessibility to highways）......

## 處理資料（Preparing the data）{-}

###  Normalizing the data{-}

```{python}
mean = train_data.mean(axis=0)
train_data -= mean # train_data = train_data - mean
std = train_data.std(axis=0)
train_data /= std # train_data = train_data / std 
test_data -= mean
test_data /= std
```

要用訓練集標準化，不能用測試集標準化。

## 建神經網絡（Building your network{-}

### Model definition{-}

```{python}
from keras import models
from keras import layers

def build_model():
  model = models.Sequential()
  model.add(layers.Dense(64, activation='relu',
                          input_shape=(train_data.shape[1],)))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(1)) # no activation
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # mean absolute error (MAE). It’s the absolute value of the difference between the predictions and the targets
  return model
```

## 驗證（Validating your approach）{-}

### K-fold validation {-}

使用K-fold validation，避免資料分成訓練集跟驗證集時，驗證集的資料點過少。

```{python}
import numpy as np
k=4
num_val_samples = len(train_data) // k # //:取整除
num_epochs = 100
all_scores = []

for i in range(k):
  print('processing fold #', i)
  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] # Prepares the validation data:data from partition #k
  val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]

  partial_train_data = np.concatenate( # Prepares the training data:data from all other partitions
    [train_data[:i * num_val_samples],
    train_data[(i + 1) * num_val_samples:]],
    axis=0)
  partial_train_targets = np.concatenate(
    [train_targets[:i * num_val_samples],
    train_targets[(i + 1) * num_val_samples:]],
    axis=0)
    
  model = build_model()
  model.fit(partial_train_data, partial_train_targets, # Trains the model(in silent mode,verbose = 0)
            epochs=num_epochs, batch_size=1, verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores.append(val_mae)
```

```{python}
print(all_scores)
print(np.mean(all_scores))
```

### Saving the validation logs at each fold{-}

```{python}
num_epochs = 500 #100改為500
all_mae_histories = []
for i in range(k):
  print('processing fold #', i)
  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]

  partial_train_data = np.concatenate(
    [train_data[:i * num_val_samples],
    train_data[(i + 1) * num_val_samples:]],
    axis=0)
  partial_train_targets = np.concatenate(
    [train_targets[:i * num_val_samples],
    train_targets[(i + 1) * num_val_samples:]],
    axis=0)
    
  model = build_model()
  history = model.fit(partial_train_data, partial_train_targets,
                      validation_data=(val_data, val_targets),
  epochs=num_epochs, batch_size=1, verbose=0)
  mae_history = history.history['val_mean_absolute_error']
  all_mae_histories.append(mae_history)
```

### Building the history of successive mean K-fold validation scores{-}

```{python}
average_mae_history = [
  np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
```

### Plotting validation scores{-}

```{python}
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()
```

### Plotting validation scores, excluding the first 10 data points{-}

Replace each point with an exponential moving average of the previous points, to obtain a smooth curve.

```{python}
def smooth_curve(points, factor=0.9):
  smoothed_points = []
  for point in points:
    if smoothed_points:
      previous = smoothed_points[-1]
      smoothed_points.append(previous * factor + point * (1 - factor))
    else:
      smoothed_points.append(point)
  return smoothed_points

smooth_mae_history = smooth_curve(average_mae_history[10:])

plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()
```

### Training the final model{-}

```{python}
model = build_model()
model.fit(train_data, train_targets,
          epochs=80, batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)
```

```{python}
print(test_mae_score)
```

# CH4{-}

# 4.1 Four branches of machine learning{-}

## Supervised learning{-}

應用在光學字元辨識、語音辨識、圖像分類、語言翻譯。非監督式學習除了分類、迴歸外，還有一些變種方式如下。

- Sequence generation：給圖片，預測描述它的標題。Sequence generation can sometimes be reformulated as a series of classification problems (such as repeatedly predicting a word or token in a sequence).

- Syntax tree prediction：給句子，predict its decomposition into a syntax
tree.

- Object detection：給圖片，在圖片中特定的目標畫出邊界。這也可表示成分類問題(given many candidate bounding boxes, classify the contents of each one)，或成為結合分類和迴歸的問題，也就是透過向量迴歸預測邊界座標。

- Image segmentation：給圖片，draw a pixel-level mask on a specific object.

## Unsupervised learning{-}

This branch of machine learning consists of finding interesting transformations of the input data without the help of any targets，用於資料視覺化(data visualization)、資料壓縮(data
compression)、資料去噪(data denoising)，或者方便理解資料中存在的相關性。

無監督式機器學習是數據分析的基礎，且在解決監督式學習的問題時，通常是更好摸索且進一步了解資料的必要步驟。像是降維(dimensionality reduction)或分群(clustering)便是常見的方法。
 
## Self-supervised learning{-}

監督式學習中一個特定的實例，自監督學習是沒有人類去標籤的監督式學習，

This is a specific instance of supervised learning, but it’s different enough that it deserves its own category. Self-supervised learning is supervised learning without human-annotated labels—you can think of it as supervised learning without any humans in the loop. There are still labels involved (because the learning has to be supervised by something), but they’re generaed from the input data, typically using a heuristic algorithm.
 For instance, autoencoders are a well-known instance of self-supervised learning, where the generated targets are the input, unmodified. In the same way, trying to predict the next frame in a video, given past frames, or the next word in a text, given previous words, are instances of self-supervised learning (temporally supervised learning, in this case: supervision comes from future input data). Note that the distinction between supervised, self-supervised, and unsupervised learning can be blurry sometimes—these categories are more of a continuum without solid borders. Self-supervised learning can be reinterpreted as either supervised or unsupervised learning, depending on whether you pay attention to the learning mechanism or to the context of its application.

## Reinforcement learning{-}

In reinforcement learning, an agent receives information about its environment and learns to choose actions that will maximize some reward. For instance, a neural network that “looks” at a videogame screen and outputs game actions in order to maximize its score can be trained via reinforcement learning.
Currently, reinforcement learning is mostly a research area and hasn’t yet had significant practical successes beyond games. In time, however, we expect to see reinforcement learning take over an increasingly large range of real-world applications: self-driving cars, robotics, resource management, education, and so on. It’s an idea whose time has come, or will come soon. 

# 4.2 Evaluating machine-learning models{-}