---
title: "Deep Learning"
author: "陳宜榛"
date: "3/19/2019"
output: html_document
---
# Environment setup{-}

```{r}
library(reticulate)
use_python("/Users/jane/anaconda3/envs/m-team-machine-learning/bin/python",
           required = T)
use_condaenv("m-team-machine-learning")
```


```{r, eval=FALSE}
conda_install(envname="m-team-machine-learning",
              c("keras","tensorflow","pip","matplotlib"),
              pip=TRUE)
```

# CH3 {-}

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(784,)))
model.add(layers.Dense(10, activation='softmax'))
```

```{python}
input_tensor = layers.Input(shape=(784,))
x = layers.Dense(32, activation='relu')(input_tensor)
output_tensor = layers.Dense(10, activation='softmax')(x)
model = models.Model(inputs=input_tensor, outputs=output_tensor)
```

```{python}
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss='mse',
metrics=['accuracy'])
```

# 3.4 二元分類範例（Classifying movie reviews:a binary classification example）

根據電影評論的文字內容，分類正面或負面的評論。

## 引入資料（The IMDB dataset）

來自電影評論網站的50,000個評論。

### Loading the IMDB dataset

```{python}
from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
num_words=10000) # 在資料中留下前10,000個最常出現的詞
```

```{python}
# train_data and test_data are lists of reviews： each review is a list of word indices (encoding a sequence of words)
print(train_data[0])
```

```{python}
# train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive
print(train_labels[0])
```

```{python}
# no word index will exceed 10,000
print(max([max(sequence) for sequence in train_data]))
```

```{python}
# how you can quickly decode one of these reviews back to English words?
word_index = imdb.get_word_index()
reverse_word_index = dict(
[(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join(
[reverse_word_index.get(i - 3, '?') for i in train_data[0]]) 
# the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
```

## 處理資料（Preparing the data）

### Encoding the integer sequences into a binary matrix

Turn the lists into tensors:

- Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the Embedding layer, which we’ll cover in detail later in the book).

- One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a Dense layer, capable of handling floating-point vector data.

```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension)) # Creates an all-zero matrix of shape (len(sequences), dimension)
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1. # Sets specific indices of results[i] to 1s
  return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
```

```{python}
print(x_train[0])
```

## 建神經網絡（Building your network）

### The model definition
 
- How many layers to use?

Two intermediate layers with 16 hidden units each
 
- How many hidden units to choose for each layer?

A third layer that will output the scalar prediction regarding the sentiment of the current review

在第四章才會解釋如何決定

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu')) # Two intermediate layers with 16 hidden units each
model.add(layers.Dense(1, activation='sigmoid')) # A third layer that will output the scalar prediction regarding the sentiment of the current review
```

NOTE:激活函數（activation functions）負責爲神經網絡引入非線性特徵。

### Compiling the model

- loss funtion:也可選擇mean_squared_error，但在二元分類中，binary_crossentropy是較好的選擇。

Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.

```{python}
model.compile(optimizer='rmsprop',
loss='binary_crossentropy',
metrics=['accuracy'])
```

### Configuring the optimizer

自行選擇optimizer的方式

```{python}
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss='binary_crossentropy',
metrics=['accuracy'])
```

### Using custom losses and metrics

自行選擇metrics的方式

```{python}
from keras import losses
from keras import metrics
model.compile(optimizer=optimizers.RMSprop(lr=0.001),
loss=losses.binary_crossentropy,
metrics=[metrics.binary_accuracy])
```

## 驗證（Validating your approach）

### Setting aside a validation set

```{python}
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```

### Training your model

```{python}
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

```{python}
history_dict = history.history
print(history_dict.keys())
```

### Plotting the training and validation loss

```{python}
import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, loss_values, 'bo', label='Training loss') # bo:blue dot
plt.plot(epochs, val_loss_values, 'b', label='Validation loss') # b:solid blue line
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```

### Plotting the training and validation accuracy

```{python}
plt.clf() # Clears the figure
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### Retraining a model from scratch

```{python}
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
loss='binary_crossentropy',
metrics=['accuracy'])
model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)
```

```{python}
print(results)
```


## 預測（Using a trained network to generate predictions on new data）

```{python}
print(model.predict(x_test))
```

## 可調參數 （Further experiments）

- Try using one or three hidden layers, and see how doing so affects validation and test accuracy.

- Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.

- Try using the `mse` loss function instead of `binary_crossentropy`.

- Try using the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.

# 3.5 多元分類範例（Classifying newswires:a multiclass classification example）

根據路透社在1986年出版的一些新聞，分類為46個主題。

## 引入資料（The Reuters dataset）

###  Loading the Reuters dataset

```{python}
from keras.datasets import reuters
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(
num_words=10000) #在資料中留下前10,000個最常出現的詞
```

```{python}
# 8,982 training examples
print(len(train_data))
```

```{python}
# 2,246 test examples
print(len(test_data))
```

```{python}
# each example is a list of integers (word indices)
print(train_data[10])
```

```{python}
# how you can decode it back to words?
word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in
  train_data[0]]) # the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
```

```{python}
print(train_labels[10])
```

## 處理資料（Preparing the data）

### Encoding the data

```{python}
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

### One-hot encoding

獨熱編碼，又稱一位有效編碼，其方法是使用N位狀態暫存器來對N個狀態進行編碼，每個狀態都有它獨立的暫存器位，並且在任意時候，其中只有一位有效。

```{python}
# 自己寫的方法
def to_one_hot(labels, dimension=46):
  results = np.zeros((len(labels), dimension))
  for i, label in enumerate(labels):
    results[i, label] = 1.
  return results
one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)
```

```{python}
# keras內建的方法
from keras.utils.np_utils import to_categorical
one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
```

## 建神經網絡（Building your network）

### Model definition

- You end the network with a `Dense` layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.

- The last layer uses a `softmax` activation. It means the network will output a probability distribution over the 46 different output classes—for every input sample, the network will produce a 46-dimensional output vector, where output[i] is the probability that the sample belongs to class i. The 46 scores will sum to 1.

```{python}
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax')) # softmax:the network will output a probability distribution over the 46 different output classes
```

### Compiling the model

```{python}
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
```

## 驗證

### Setting aside a validation set

```{python}
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
```

### Training the model

```{python}
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

### Plotting the training and validation loss

```{python}
import matplotlib.pyplot as plt
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### Plotting the training and validation accuracy

```{python}
plt.clf()
acc = history.history['acc']
val_acc = history.history['val_acc']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

### Retraining a model from scratch

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(partial_x_train,
partial_y_train,
epochs=9,
batch_size=512,
validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)
```

```{python}
print(results)
```

```{python}
import copy
test_labels_copy = copy.copy(test_labels)
np.random.shuffle(test_labels_copy)
hits_array = np.array(test_labels) == np.array(test_labels_copy)
print(float(np.sum(hits_array)) / len(test_labels))
```

## 預測（Using a trained network to generate predictions on new data）

```{python}
predictions = model.predict(x_test)
```

```{python}
# Each entry in predictions is a vector of length 46
print(predictions[0].shape)
```

```{python}
# The coefficients in this vector sum to 1
print(np.sum(predictions[0]))
```

```{python}
# The largest entry is the predicted class—the class with the highest probability
print(np.argmax(predictions[0]))
```

## 可調參數 （Further experiments）

- Try using larger or smaller layers: 32 units, 128 units, and so on.

- Try using a single hidden layer, or three hidden layers.

## 其他課題

### A different way to handle the labels and the loss

```{python}
y_train = np.array(train_labels)
y_test = np.array(test_labels)
```

```{python}
model.compile(optimizer='rmsprop',
loss='sparse_categorical_crossentropy',
metrics=['acc'])
```

### The importance of having sufficiently large intermediate layers

drop掉太多內容，會導致準確率下降

```{python}
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
model.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(partial_x_train,
partial_y_train,
epochs=20,
batch_size=128,
validation_data=(x_val, y_val))
```

# 3.6 迴歸範例（Predicting house prices: a regression example）

## 引入資料（The Boston Housing Price dataset）

###  Loading the Boston housing dataset