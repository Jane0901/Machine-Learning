---
title: "Unsupervised-Learning"
author: "陳宜榛"
date: "1/12/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 非監督式學習（Unsupervised Learning）

### 1. 概念

其目的是去對原始資料進行分類，以便了解資料內部結構。有別於監督式學習網絡，無監督式學習網絡在學習時並不知道其分類結果是否正確，亦即沒有受到監督式增強(告訴它何種學習是正確的)。其特點是僅對此種網絡提供輸入範例。而它會自己主動從這些範例中找出其潛在類別規則。當學習完成並經測試後，也能夠將之應用到新的案例上。

* 監督式學習：資料已有標記，運用已標記資料來做訓練，所以模型評估講求準度。
* 非監督式學習：資料沒有標記，從中找出擁有相同特徵的資料群，所以是找出資料間大致分布的趨勢，而沒有要預測的對象。

### 2. 常使用的方法

* 分群法
* 學習潛在變數模型的方法
* 人工神經網路
* 異常檢測

參考資料：[維基百科 非監督式學習](https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92)

## 分群（Clustering）

### 1. 概念

分群是一種將資料分類成群的方法，為一種非監督式學習，也就是訓練資料沒有預先定義的標籤。其主要的目的在於找出資料中相似的幾個群聚，讓在同一個子集中的成員對象都有相似的一些屬性，常見的包括在坐標系中更加短的空間距離等。

一般而言，分群法可以大致歸為兩大類：

* 階層式分群法 (hierarchical clustering) : 群的數目可以由大變小(divisive hierarchical clustering)，或是由小變大(agglomerative hierarchical clustering)，來進群聚的合併或分裂，最後再選取最佳的群的數目。
* 分割式分群法 (partitional clustering) : 先指定群的數目後，再用一套疊代的數學運算法，找出最佳的分群方式以及相關的群中心。

常見的分法有K-平均演算法、階層式分群、混合模型。

### 2. K-平均演算法

參考資料：<http://mropengate.blogspot.com/2015/06/ai-ch16-5-k-introduction-to-clustering.html>、<https://zh.wikipedia.org/wiki/K-%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95>

#### 2-1 概念

把n個點劃分到k個聚類中，使得每個點都屬於離他最近的均值(聚類中心)對應的聚類，以之作為聚類的標準。

這個問題在計算上是 [NP困難](https://zh.wikipedia.org/wiki/NP%E5%9B%B0%E9%9A%BE)，不過存在高效的 [啟發式演算法](https://zh.wikipedia.org/wiki/%E5%90%AF%E5%8F%91%E5%BC%8F%E6%90%9C%E7%B4%A2)。一般情況下，都使用效率比較高的啟發式演算法，它們能夠快速收斂於一個局部最優解。

這些演算法通常類似於通過疊代最佳化方法處理高斯混合分布的最大期望演算法（EM演算法）。而且，它們都使用聚類中心來為資料建模；然而k-平均聚類傾向於在可比較的空間範圍內尋找聚類，期望-最大化技術卻允許聚類有不同的形狀。

>聚類數目k是一個輸入參數，通常要進行特徵檢查以決定聚類數目。

#### 2-2 演算法描述

#### 2-3 優缺點

* 優點：此演算法的效率很高。
* 缺點：

1. 聚類數目k是一個輸入參數，選擇不恰當的k值可能會導致糟糕的聚類結果。
2. 收斂到局部最佳解，可能導致「反直觀」的錯誤結果。

### 3. 階層式分群

參考資料：<http://mropengate.blogspot.com/2015/06/ai-ch17-6-clustering-hierarchical.html>

### 4. 混合模型

## 學習潛在變數模型的方法（Latent Variable Models）

## 圖形結構（Graphical Structure）

## 總結
